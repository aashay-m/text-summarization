{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerDecoder,TransformerDecoderLayer\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.nn import Transformer\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mapka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from functools import lru_cache\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.utils as utils\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "cached_lemmatize = lru_cache(maxsize=50000)(WordNetLemmatizer().lemmatize)\n",
    "from gensim.utils import simple_preprocess, to_unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_dim,emb_dim,enc_hid_dim,dec_hid_dim,dropout=0.5):\n",
    "        \n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim,emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        \n",
    "        self.fc = nn.Linear( enc_hid_dim * 2, dec_hid_dim )\n",
    "        \n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(X))\n",
    "        \n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        \n",
    "        hidden = F.tanh( self.fc ( torch.cat( (hidden[-2,:,:], hidden[-1, : , : ] ), dim = 1 ) ) )\n",
    "        \n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"data\"\n",
    "train_file_X = os.path.join(base_dir,\"train.source\")\n",
    "train_file_y = os.path.join(base_dir,\"train.target\")\n",
    "test_file_X = os.path.join(base_dir,\"test.source\")\n",
    "test_file_y = os.path.join(base_dir,\"test.target\")\n",
    "val_file_X = os.path.join(base_dir,\"val.source\")\n",
    "val_file_y = os.path.join(base_dir,\"val.target\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "STOP_WORDS = [\"i\", \"a\", \"about\", \"an\", \"are\", \"as\", \"at\", \"be\", \"by\", \n",
    "                \"for\", \"from\", \"how\", \"in\", \"is\", \"it\", \"of\", \"on\", \"or\", \"that\", \"the\", \n",
    "                \"this\", \"to\", \"was\", \"what\", \"when\", \"where\", \"who\", \"will\", \"with\"]\n",
    "\n",
    "def ExpandContractions(contraction):\n",
    "\n",
    "    contraction = re.sub(r\"won\\'t\", \"will not\", contraction)\n",
    "    contraction = re.sub(r\"can\\'t\", \"can not\", contraction)\n",
    "\n",
    "    contraction = re.sub(r\"n\\'t\", \" not\", contraction)\n",
    "    contraction = re.sub(r\"\\'re\", \" are\", contraction)\n",
    "    contraction = re.sub(r\"\\'s\", \" is\", contraction)\n",
    "    contraction = re.sub(r\"\\'d\", \" would\", contraction)\n",
    "    contraction = re.sub(r\"\\'ll\", \" will\", contraction)\n",
    "    contraction = re.sub(r\"\\'t\", \" not\", contraction)\n",
    "    contraction = re.sub(r\"\\'ve\", \" have\", contraction)\n",
    "    contraction = re.sub(r\"\\'m\", \" am\", contraction)\n",
    "\n",
    "    return contraction\n",
    "\n",
    "def PreProcess(line):\n",
    "    \n",
    "    line = line.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    line = ExpandContractions(line)\n",
    "    line = simple_preprocess(to_unicode(line))\n",
    "    line = [cached_lemmatize(word) for word in line if word not in STOP_WORDS]\n",
    "\n",
    "    line = \" \".join(line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineSentenceGenerator(object):\n",
    "\n",
    "    def __init__(self, source, preprocess=None, max_sentence_length=4000, limit=None, preprocess_flag=True):\n",
    "        self.source = source\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.limit = limit\n",
    "        self.input_files = []\n",
    "\n",
    "        if preprocess != None and callable(preprocess) and preprocess_flag:\n",
    "            self.preprocess = preprocess\n",
    "        else:\n",
    "            self.preprocess = lambda line: line.rstrip(\"\\r\\n\")\n",
    "\n",
    "        if isinstance(self.source, list):\n",
    "            print('List of files given as source. Verifying entries and using.')\n",
    "            self.input_files = [filename for filename in self.source if os.path.isfile(filename)]\n",
    "            self.input_files.sort()  # makes sure it happens in filename order\n",
    "\n",
    "        elif os.path.isfile(self.source):\n",
    "            print('Single file given as source, rather than a list of files. Wrapping in list.')\n",
    "            self.input_files = [self.source]  # force code compatibility with list of files\n",
    "\n",
    "        elif os.path.isdir(self.source):\n",
    "            self.source = os.path.join(self.source, '')  # ensures os-specific slash at end of path\n",
    "            print('Directory of files given as source. Reading directory %s', self.source)\n",
    "            self.input_files = os.listdir(self.source)\n",
    "            self.input_files = [self.source + filename for filename in self.input_files]  # make full paths\n",
    "            self.input_files.sort()  # makes sure it happens in filename order\n",
    "        else:  # not a file or a directory, then we can't do anything with it\n",
    "            raise ValueError('Input is neither a file nor a path nor a list')\n",
    "        print('Files read into LineSentenceGenerator: %s' % ('\\n'.join(self.input_files)))\n",
    "\n",
    "        self.token_count = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        for file_name in self.input_files:\n",
    "            print('Reading file %s', file_name)\n",
    "            with open(file_name, 'rb') as fin:\n",
    "                for line in itertools.islice(fin, self.limit):\n",
    "                    line = self.preprocess(utils.to_unicode(line))\n",
    "                    self.token_count += len(line)\n",
    "                    i = 0\n",
    "                    while i < len(line):\n",
    "                        yield line[i:i + self.max_sentence_length]\n",
    "                        i += self.max_sentence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.token_count > 0:\n",
    "            return self.token_count\n",
    "        else:\n",
    "            return len(self.input_files)\n",
    "\n",
    "    def __bool__(self):\n",
    "        return self.has_data()\n",
    "\n",
    "    def is_empty(self):\n",
    "        return len(self.input_files) == 0\n",
    "\n",
    "    def has_data(self):\n",
    "        return not self.is_empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Dataset,Example\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "SRC = Field(tokenize = get_tokenizer(\"spacy\"),\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(X, y, src, preprocess=None, limit=1000):\n",
    "    examples = []\n",
    "    fields = {'text-tokens': ('text', src),\n",
    "              'summ-tokens': ('summ', src)}\n",
    "    for i,(x,y) in enumerate(zip(LineSentenceGenerator(X, preprocess),LineSentenceGenerator(y, preprocess))):\n",
    "        text_field = x\n",
    "        summ_field = y\n",
    "        \n",
    "        if i > limit:\n",
    "            break\n",
    "       \n",
    "        e = Example.fromdict({\"text-tokens\": text_field, \"summ-tokens\": summ_field},\n",
    "                             fields=fields)\n",
    "        examples.append(e)\n",
    "    print(\"examples: \\n\", examples[0])\n",
    "    return Dataset(examples, fields=[('text', src), ('summ', src)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single file given as source, rather than a list of files. Wrapping in list.\n",
      "Files read into LineSentenceGenerator: data\\train.source\n",
      "Single file given as source, rather than a list of files. Wrapping in list.\n",
      "Files read into LineSentenceGenerator: data\\train.target\n",
      "Reading file %s data\\train.source\n",
      "Reading file %s data\\train.target\n",
      "examples: \n",
      " <torchtext.data.example.Example object at 0x0000022F73E5BB08>\n",
      "Single file given as source, rather than a list of files. Wrapping in list.\n",
      "Files read into LineSentenceGenerator: data\\test.source\n",
      "Single file given as source, rather than a list of files. Wrapping in list.\n",
      "Files read into LineSentenceGenerator: data\\test.target\n",
      "Reading file %s data\\test.source\n",
      "Reading file %s data\\test.target\n",
      "examples: \n",
      " <torchtext.data.example.Example object at 0x0000022F18762288>\n",
      "Single file given as source, rather than a list of files. Wrapping in list.\n",
      "Files read into LineSentenceGenerator: data\\val.source\n",
      "Single file given as source, rather than a list of files. Wrapping in list.\n",
      "Files read into LineSentenceGenerator: data\\val.target\n",
      "Reading file %s data\\val.source\n",
      "Reading file %s data\\val.target\n",
      "examples: \n",
      " <torchtext.data.example.Example object at 0x0000022F18BCCA48>\n"
     ]
    }
   ],
   "source": [
    "train_data = read_data(train_file_X,train_file_y,SRC,PreProcess,1000)\n",
    "test_data = read_data(test_file_X,test_file_y,SRC,PreProcess,200)\n",
    "val_data = read_data(val_file_X,val_file_y,SRC,PreProcess,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  ['editor', 'note', 'our', 'behind', 'scene', 'series', 'cnn', 'correspondent', 'share', 'their', 'experience', 'covering', 'news', 'and', 'analyze', 'story', 'behind', 'event', 'here', 'soledad', 'obrien', 'take', 'user', 'inside', 'jail', 'many', 'inmate', 'mentally', 'ill', 'inmate', 'housed', 'forgotten', 'floor', 'many', 'mentally', 'ill', 'inmate', 'housed', 'miami', 'before', 'trial', 'miami', 'florida', 'cnn', 'ninth', 'floor', 'miamidade', 'pretrial', 'detention', 'facility', 'dubbed', 'forgotten', 'floor', 'here', 'inmate', 'most', 'severe', 'mental', 'illness', 'incarcerated', 'until', 'they', 're', 'ready', 'appear', 'court', 'most', 'often', 'they', 'face', 'drug', 'charge', 'charge', 'assaulting', 'officer', 'charge', 'judge', 'steven', 'leifman', 'say', 'usually', 'avoidable', 'felony', 'he', 'say', 'arrest', 'often', 'result', 'confrontation', 'police', 'mentally', 'ill', 'people', 'often', 'wo', 'nt', 'do', 'they', 're', 'told', 'police', 'arrive', 'scene', 'confrontation', 'seems', 'exacerbate', 'their', 'illness', 'and', 'they', 'become', 'more', 'paranoid', 'delusional', 'and', 'le', 'likely', 'follow', 'direction', 'according', 'leifman', 'so', 'they', 'end', 'up', 'ninth', 'floor', 'severely', 'mentally', 'disturbed', 'but', 'not', 'getting', 'any', 'real', 'help', 'because', 'they', 're', 'jail', 'we', 'toured', 'jail', 'leifman', 'he', 'well', 'known', 'miami', 'advocate', 'justice', 'and', 'mentally', 'ill', 'even', 'though', 'we', 'were', 'not', 'exactly', 'welcomed', 'open', 'arm', 'guard', 'we', 'were', 'given', 'permission', 'shoot', 'videotape', 'and', 'tour', 'floor', 'go', 'inside', 'forgotten', 'floor', 'first', 'it', 'hard', 'determine', 'people', 'prisoner', 'wearing', 'sleeveless', 'robe', 'imagine', 'cutting', 'hole', 'arm', 'and', 'foot', 'heavy', 'wool', 'sleeping', 'bag', 'that', 's', 'kind', 'they', 'look', 'like', 'they', 're', 'designed', 'keep', 'mentally', 'ill', 'patient', 'injuring', 'themselves', 'that', 's', 'also', 'why', 'they', 'have', 'no', 'shoe', 'lace', 'mattress', 'leifman', 'say', 'onethird', 'all', 'people', 'miamidade', 'county', 'jail', 'mentally', 'ill', 'so', 'he', 'say', 'sheer', 'volume', 'overwhelming', 'system', 'and', 'result', 'we', 'see', 'ninth', 'floor', 'course', 'jail', 'so', 'it', 'not', 'supposed', 'warm', 'and', 'comforting', 'but', 'light', 'glare', 'cell', 'tiny', 'and', 'it', 'loud', 'we', 'see', 'two', 'sometimes', 'three', 'men', 'sometimes', 'robe', 'sometimes', 'naked', 'lying', 'sitting', 'their', 'cell', 'am', 'son', 'president', 'you', 'need', 'get', 'me', 'out', 'here', 'one', 'man', 'shout', 'me', 'he', 'absolutely', 'serious', 'convinced', 'help', 'way', 'if', 'only', 'he', 'could', 'reach', 'white', 'house', 'leifman', 'tell', 'me', 'these', 'often', 'circulate', 'through', 'system', 'occasionally', 'stabilizing', 'mental', 'hospital', 'only', 'return', 'jail', 'face', 'their', 'charge', 'it', 'brutally', 'unjust', 'his', 'mind', 'and', 'he', 'ha', 'become', 'strong', 'advocate', 'changing', 'thing', 'miami', 'over', 'meal', 'later', 'we', 'talk', 'thing', 'got', 'way', 'mental', 'patient', 'leifman', 'say', 'year', 'ago', 'people', 'were', 'considered', 'lunatic', 'and', 'they', 'were', 'locked', 'up', 'jail', 'even', 'if', 'they', 'had', 'no', 'charge', 'against', 'them', 'they', 'were', 'just', 'considered', 'unfit', 'society', 'over', 'year', 'he', 'say', 'there', 'some', 'public', 'outcry', 'and', 'mentally', 'ill', 'were', 'moved', 'out', 'jail', 'and', 'into', 'hospital', 'but', 'leifman', 'say', 'many', 'these', 'mental', 'hospital', 'were', 'so', 'horrible', 'they', 'were', 'shut', 'down', 'did', 'patient', 'go', 'nowhere', 'street', 'they', 'became', 'many', 'case', 'homeless', 'he', 'say', 'they', 'never', 'got', 'treatment', 'leifman', 'say', 'there', 'were', 'more', 'than', 'half', 'million', 'people', 'state', 'mental', 'hospital', 'and', 'today', 'number', 'ha', 'been', 'reduced', 'percent', 'and', 'people', 'mental', 'hospital', 'judge', 'say', 'he', 'working', 'change', 'starting', 'many', 'inmate', 'would', 'otherwise', 'have', 'been', 'brought', 'forgotten', 'floor', 'instead', 'sent', 'new', 'mental', 'health', 'facility', 'first', 'step', 'journey', 'toward', 'longterm', 'treatment', 'not', 'just', 'punishment', 'leifman', 'say', 'it', 'not', 'complete', 'answer', 'but', 'it', 'start', 'leifman', 'say', 'best', 'part', 'it', 'winwin', 'solution', 'patient', 'win', 'family', 'relieved', 'and', 'state', 'save', 'money', 'simply', 'not', 'cycling', 'these', 'prisoner', 'through', 'again', 'and', 'again', 'and', 'leifman', 'justice', 'served', 'email', 'friend']\n",
      "\n",
      "text-len:  510\n",
      "\n",
      "\n",
      "summary:  ['mentally', 'ill', 'inmate', 'miami', 'housed', 'forgotten', 'floor', 'judge', 'steven', 'leifman', 'say', 'most', 'there', 'result', 'avoidable', 'felony', 'while', 'cnn', 'tour', 'facility', 'patient', 'shout', 'am', 'son', 'president', 'leifman', 'say', 'system', 'unjust', 'and', 'he', 'fighting', 'change']\n"
     ]
    }
   ],
   "source": [
    "print(\"text: \",train_data[0].text)\n",
    "print(\"\\ntext-len: \",len(train_data[0].text))\n",
    "print(\"\\n\\nsummary: \",train_data[0].summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': <torchtext.data.field.Field at 0x22f625f8b08>,\n",
       " 'summ': <torchtext.data.field.Field at 0x22f625f8b08>}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  ['cnna', 'frenchlanguage', 'global', 'television', 'network', 'regained', 'control', 'one', 'it', 'channel', 'thursday', 'after', 'cyberattack', 'day', 'earlier', 'crippled', 'it', 'broadcast', 'and', 'social', 'medium', 'account', 'television', 'network', 'tv', 'monde', 'gradually', 'regaining', 'control', 'it', 'channel', 'and', 'social', 'medium', 'outlet', 'after', 'suffering', 'network', 'director', 'called', 'extremely', 'powerful', 'cyberattack', 'addition', 'it', 'channel', 'tv', 'monde', 'lost', 'control', 'it', 'social', 'medium', 'outlet', 'and', 'it', 'website', 'director', 'yves', 'bigot', 'said', 'video', 'message', 'posted', 'later', 'facebook', 'mobile', 'site', 'which', 'still', 'active', 'network', 'said', 'hacked', 'islamist', 'group', 'isi', 'logo', 'and', 'marking', 'appeared', 'tv', 'monde', 'social', 'medium', 'account', 'but', 'there', 'no', 'immediate', 'claim', 'responsibility', 'isi', 'any', 'other', 'group', 'day', 'broke', 'thursday', 'europe', 'network', 'had', 'regained', 'use', 'one', 'it', 'channel', 'and', 'it', 'facebook', 'page', 'paul', 'germain', 'chain', 'editor', 'chief', 'told', 'bfmtv', 'cnn', 'affiliate', 'france', 'however', 'late', 'morning', 'number', 'page', 'network', 'website', 'had', 'message', 'saying', 'they', 'were', 'under', 'maintenance', 'outage', 'began', 'around', 'pm', 'paris', 'time', 'pm', 'et', 'wednesday', 'tv', 'monde', 'offer', 'roundtheclock', 'entertainment', 'and', 'news', 'programming', 'reach', 'million', 'home', 'worldwide', 'according', 'ministry', 'culture', 'and', 'communication', 'function', 'under', 'partnership', 'among', 'government', 'france', 'canada', 'and', 'switzerland', 'well', 'federation', 'other', 'network', 'provide', 'content', 'tv', 'monde', 'include', 'cnn', 'affiliate', 'france', 'and', 'france', 'france', 'and', 'radio', 'france', 'international']\n",
      "\n",
      "\n",
      "summ:  ['don', 'mcleans', 'american', 'pie', 'lyric', 'auctioned', 'million', 'song', 'dense', 'symbolism', 'mclean', 'say', 'lyric', 'note', 'reveal', 'meaning', 'pie', 'mcleans', 'biggest', 'hit', 'no']\n"
     ]
    }
   ],
   "source": [
    "print(\"text: \", test_data[100].text)\n",
    "print(\"\\n\\nsumm: \",test_data[100].summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data.text, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iter = BucketIterator(train_data,BATCH_SIZE, shuffle=True,\n",
    "                                                 sort_key=lambda x: len(x.text), sort_within_batch=True)\n",
    "\n",
    "val_iter = BucketIterator(val_data, BATCH_SIZE, sort_key=lambda x: len(x.text), sort_within_batch=True)\n",
    "test_iter = BucketIterator(test_data,BATCH_SIZE, sort_key=lambda x: len(x.text), sort_within_batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class TransformerSummarizer(nn.Module):\n",
    "    def __init__(self, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length,vocab_size, pad_idx,  d_model=None, pos_dropout =0.1, trans_dropout= 0.1,embeddings=None):\n",
    "        super().__init__()\n",
    "       \n",
    "        if embeddings is None:\n",
    "            self.embed_src = nn.Embedding(vocab_size, d_model)\n",
    "            self.embed_tgt = nn.Embedding(vocab_size, d_model)\n",
    "        else:\n",
    "            d_model = embeddings.size(1)\n",
    "            self.d_model = embeddings.size(1)\n",
    "            self.embed_src = nn.Embedding(*embeddings.shape)\n",
    "            self.embed_src.weight = nn.Parameter(embeddings,requires_grad=False)\n",
    "            \n",
    "            self.embed_tgt = nn.Embedding(*embeddings.shape)\n",
    "            self.embed_tgt.weight = nn.Parameter(embeddings,requires_grad=False)\n",
    "        \n",
    "        \n",
    "        self.pos_enc = PositionalEncoding(d_model, pos_dropout, max_seq_length)\n",
    "\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, trans_dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        self.src_mask = None\n",
    "        self.tgt_mask = None\n",
    "        self.memory_mask = None\n",
    "        \n",
    "    def generate_square_mask(self,sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    def make_pad_mask(self,seq,pad_idx):\n",
    "        mask = (seq == pad_idx).transpose(0,1)\n",
    "        return mask\n",
    "    \n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        if self.tgt_mask is None or self.tgt_mask.size(0) != len(trg):\n",
    "            self.trg_mask = self.generate_square_mask(len(trg)).to(trg.device)\n",
    "        \n",
    "        \n",
    "        src_pad_mask = self.make_pad_mask(src,self.pad_idx)\n",
    "        tgt_pad_mask = self.make_pad_mask(tgt,self.pad_idx)\n",
    "\n",
    "        \n",
    "        src = self.pos_enc(self.embed_src(src) * math.sqrt(self.d_model))\n",
    "\n",
    "        tgt = self.pos_enc(self.embed_tgt(tgt) * math.sqrt(self.d_model))\n",
    "        \n",
    "\n",
    "        output = self.transformer(src, tgt, src_mask=self.src_mask, tgt_mask=self.tgt_mask, memory_mask=self.memory_mask, \n",
    "                                 src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask, memory_key_padding_mask=src_pad_mask)\n",
    "        \n",
    "        return self.fc(output)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab-size:  14744\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SEQ_LEN = 4000\n",
    "\n",
    "D_MODEL = 300 # Embedding dimension\n",
    "DIM_FEEDFORWARD = 300  # Dimensionality of the hidden state\n",
    "VOCAB_SIZE = len(SRC.vocab)  # size of the vocabulary\n",
    "print(\"vocab-size: \", VOCAB_SIZE) \n",
    "\n",
    "ATTENTION_HEADS = 6  # number of attention heads\n",
    "N_LAYERS = 1 # number of encoder/decoder layers\n",
    "\n",
    "\n",
    "\n",
    "# nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length,vocab_size, pad_idx,  d_model=None, pos_dropout =0.1, trans_dropout= 0.1,embeddings=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14744, 300])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.vocab import FastText\n",
    "\n",
    "ff = FastText(\"en\")\n",
    "\n",
    "embeddings =  ff.get_vecs_by_tokens(SRC.vocab.itos)\n",
    "\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerSummarizer( ATTENTION_HEADS,N_LAYERS, N_LAYERS, DIM_FEEDFORWARD, SEQ_LEN,VOCAB_SIZE,PAD_IDX,embeddings=embeddings).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerSummarizer(\n",
       "  (embed_src): Embedding(14744, 300)\n",
       "  (embed_tgt): Embedding(14744, 300)\n",
       "  (pos_enc): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=300, out_features=14744, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "def train(model: nn.Module,\n",
    "          iterator: BucketIterator,\n",
    "          num_batches: int,\n",
    "          optimizer: optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float):\n",
    "    \n",
    "    print(\"Training......\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in tqdm(iterator,total=num_batches):\n",
    "        src = batch.text\n",
    "        trg = batch.summ\n",
    "        \n",
    "\n",
    "        trg_inp, trg_out = trg[:-1, :], trg[1:, :]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src.to(device), trg_inp.to(device))\n",
    "    \n",
    "        output = output.view(-1, output.shape[-1])\n",
    "\n",
    "        loss = criterion(output, trg_out.view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    print(\"Training Done.....\")\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module,\n",
    "             iterator: BucketIterator,\n",
    "             num_batches:int,\n",
    "             criterion: nn.Module,\n",
    "            desc: str):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    print(f'{desc}')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in tqdm(iterator,total=num_batches):\n",
    "            \n",
    "            src = batch.text\n",
    "            trg = batch.summ\n",
    "            \n",
    "            trg_inp, trg_out = trg[:-1, :], trg[1:, :]\n",
    "\n",
    "            output = model(src.to(device), trg_inp.to(device))\n",
    "\n",
    "            output = output.view(-1,output.shape[-1])\n",
    "\n",
    "            loss = criterion(output, trg_out.view(-1))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        print(f\"{desc} Done........\")\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_list = SRC.vocab.itos  # index2word\n",
    "src_dict = SRC.vocab.stoi # word2index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training......\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b051f615eb2400084ca8de83b938603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Done.....\n",
      "evaluating\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299e1c0a5ebe4267810ec4f8eccbc39b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "evaluating Done........\n",
      "Epoch: 01 | Time: 2m 14s\n",
      "\tTrain Loss: 8.879\n",
      "\t Val. Loss: 7.641\n",
      "testing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601025f5ab3d414f88dba007b5521059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "testing Done........\n",
      "| Test Loss: 7.675\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def epoch_time(start_time: int,\n",
    "               end_time: int):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "parameters = filter(lambda p:p.requires_grad, model.parameters())\n",
    "optimizer = optim.Adam(parameters)\n",
    "num_batches = math.ceil(len(train_data)/BATCH_SIZE)\n",
    "val_batches = math.ceil(len(val_data)/BATCH_SIZE)\n",
    "\n",
    "N_EPOCHS = 1\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_iter, num_batches,optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, val_iter,val_batches, criterion, \"evaluating\")\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f}')\n",
    "    \n",
    "test_size = math.ceil(len(test_data)/BATCH_SIZE)\n",
    "test_loss = evaluate(model, test_iter,test_size, criterion, \"testing\")\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  ['<sos>', 'cnn', 'red', 'cross', 'president', 'and', 'ceo', 'mark', 'everson', 'ha', 'stepped', 'down', 'after', 'revelation', 'he', 'engaged', 'personal', 'relationship', 'subordinate', 'employee', 'organization', 'announced', 'tuesday', 'mark', 'everson', 'say', 'he', 'leaving', 'his', 'post', 'effective', 'immediately', 'personal', 'and', 'family', 'reason', 'red', 'cross', 'board', 'governor', 'asked', 'and', 'received', 'eversons', 'resignation', 'after', 'concluded', 'situation', 'reflected', 'poor', 'judgment', 'mr', 'eversons', 'part', 'and', 'diminished', 'his', 'ability', 'lead', 'organization', 'future', 'red', 'cross', 'said', 'statement', 'it', 'web', 'site', 'everson', 'said', 'written', 'statement', 'he', 'leaving', '<unk>', 'job', 'personal', 'and', 'family', 'reason', 'and', 'deeply', 'regret', 'impossible', 'me', 'continue', 'job', 'so', 'recently', '<unk>', 'everson', 'married', 'and', 'ha', 'two', 'child', 'joined', 'red', 'cross', 'president', 'and', 'ceo', 'last', 'may', 'organization', 'became', 'aware', 'eversons', 'relationship', 'female', 'red', 'cross', 'employee', 'day', 'ago', 'chief', 'public', 'affair', 'officer', '<unk>', 'defrancis', 'told', 'cnn', 'telephone', 'interview', 'think', 'board', 'acted', 'very', 'quickly', 'she', 'said', 'adding', 'woman', 'remains', 'her', 'job', 'everson', 'defrancis', 'said', 'were', 'grateful', 'his', 'service', 'board', 'governor', 'tuesday', 'appointed', 'mary', '<unk>', 'general', 'counsel', 'and', 'fiveyear', 'red', 'cross', 'employee', 'interim', 'president', 'and', 'ceo', 'everson', 'had', 'worked', 'bush', 'administration', 'august', 'including', 'serving', 'commissioner', 'internal', 'revenue', 'service', 'until', 'he', 'hired', 'red', 'cross', '<unk>', 'that', 's', 'all', 'can', 'say', 'it', 'completely', 'contrary', 'his', 'public', 'persona', 'he', 'evidenced', 'while', 'he', 'irs', 'said', 'suzanne', 'ross', '<unk>', 'washingtonbased', 'attorney', 'served', 'advisory', 'committee', 'irs', 'division', 'deal', '<unk>', 'organization', 'standpoint', 'exempt', 'organization', 'nonprofit', 'sector', 'it', 'just', 'another', 'news', 'story', 'we', 'would', 'rather', 'not', 'see', 'she', 'said', 'it', 'got', 'nothing', 'do', 'red', 'cross', 'said', 'ira', '<unk>', 'new', 'york', 'lawyer', 'specializing', 'corporate', 'governance', 'ha', 'worked', 'organization', 'and', 'impressed', 'everson', 'he', 'team', 'player', 'and', 'good', 'leader', 'have', 'him', 'fall', 'off', 'cliff', 'like', 'just', 'sad', 'search', 'committee', 'ha', 'been', 'formed', 'begin', 'process', 'finding', 'eversons', 'permanent', 'replacement', 'organization', 'said', 'job', 'ha', 'been', 'challenging', 'one', 'marsha', 'evans', 'resigned', 'president', 'after', 'red', 'cross', 'response', 'hurricane', 'katrina', 'came', 'under', 'fire', 'four', 'year', 'earlier', '<unk>', 'healy', 'quit', 'post', 'after', 'organization', 'criticized', '<unk>', 'donation', 'intended', 'victim', 'september', 'terrorist', 'attack', 'and', 'collecting', 'vast', 'quantity', 'blood', 'not', 'needed', 'and', 'ultimately', 'thrown', 'out', 'healy', 'told', 'reporter', 'she', 'had', 'no', 'choice', 'her', 'resignation', 'meanwhile', 'defrancis', 'acknowledged', 'tuesday', 'year', 'after', 'court', 'ordered', 'agency', 'improve', 'it', 'collection', 'blood', 'ha', 'yet', 'meet', 'federal', 'safety', 'and', '<unk>', 'requirement', 'email', 'friend', '<eos>', '<pad>']\n",
      "\n",
      "\n",
      "summ:  ['<sos>', 'suicide', 'car', 'bomber', 'targeted', 'danish', 'embassy', 'capital', 'islamabad', '<unk>', 'account', 'put', 'death', 'toll', 'between', 'eight', 'and', 'six', 'people', 'medical', 'worker', 'say', 'child', 'and', 'least', 'one', 'foreign', 'national', 'died', 'blast', 'danish', 'embassy', 'scene', 'protest', 'last', 'year', 'over', '<unk>', 'cartoon', 'row', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "out:\n",
      " tensor([[[ 6.0307, -3.5627, -4.3091,  ..., -2.9609, -3.6032, -3.8193],\n",
      "         [ 6.0661, -3.5324, -4.3047,  ..., -3.0001, -3.5479, -3.8325],\n",
      "         [ 6.0088, -3.5212, -4.3316,  ..., -2.8538, -3.5209, -3.8411],\n",
      "         ...,\n",
      "         [ 5.9381, -3.5761, -4.2955,  ..., -2.9175, -3.6465, -3.8124],\n",
      "         [ 5.9901, -3.5485, -4.4104,  ..., -2.9781, -3.5742, -3.8171],\n",
      "         [ 6.1391, -3.5979, -4.2624,  ..., -2.9378, -3.5639, -3.8187]],\n",
      "\n",
      "        [[ 6.0044, -3.6194, -4.2299,  ..., -2.8398, -3.6449, -3.9769],\n",
      "         [ 5.8735, -3.5092, -4.5614,  ..., -3.0214, -3.6068, -4.0458],\n",
      "         [ 5.6966, -3.5396, -4.3078,  ..., -2.8194, -3.5887, -4.0035],\n",
      "         ...,\n",
      "         [ 5.8157, -3.6216, -4.2445,  ..., -3.0643, -3.5124, -4.1090],\n",
      "         [ 5.9738, -3.5037, -4.3287,  ..., -2.9844, -3.4945, -4.0075],\n",
      "         [ 5.9322, -3.6112, -4.2002,  ..., -2.9734, -3.4842, -4.0400]],\n",
      "\n",
      "        [[ 6.0124, -3.7089, -4.1416,  ..., -2.9542, -3.6268, -4.1706],\n",
      "         [ 5.8674, -3.4626, -4.3592,  ..., -3.1964, -3.5405, -3.8882],\n",
      "         [ 5.8188, -3.5742, -4.1351,  ..., -2.9325, -3.4938, -3.9612],\n",
      "         ...,\n",
      "         [ 5.8819, -3.5517, -4.0773,  ..., -2.8722, -3.5437, -4.0430],\n",
      "         [ 5.9108, -3.4684, -4.3613,  ..., -3.0916, -3.4389, -3.9285],\n",
      "         [ 5.9479, -3.6696, -4.2985,  ..., -2.9862, -3.4257, -4.1683]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 5.7275, -3.5207, -4.2432,  ..., -2.9320, -3.5956, -3.8782],\n",
      "         [ 6.0365, -3.5348, -4.3215,  ..., -3.0405, -3.5330, -3.8296],\n",
      "         [ 5.9511, -3.5381, -4.3743,  ..., -2.9051, -3.4840, -3.8754],\n",
      "         ...,\n",
      "         [ 5.9630, -3.5983, -4.1680,  ..., -2.9267, -3.5608, -3.8478],\n",
      "         [ 5.9916, -3.5796, -4.3347,  ..., -2.9219, -3.5743, -3.8680],\n",
      "         [ 6.1583, -3.5869, -4.2954,  ..., -2.9409, -3.5735, -3.8523]],\n",
      "\n",
      "        [[ 5.8881, -3.7234, -4.1579,  ..., -2.9147, -3.7100, -3.9882],\n",
      "         [ 6.0279, -3.5439, -4.3340,  ..., -3.0587, -3.5312, -3.8333],\n",
      "         [ 5.9622, -3.5251, -4.3648,  ..., -2.9181, -3.4863, -3.8870],\n",
      "         ...,\n",
      "         [ 5.9491, -3.6044, -4.1468,  ..., -2.9340, -3.5470, -3.8644],\n",
      "         [ 5.9870, -3.5818, -4.3330,  ..., -2.9334, -3.5702, -3.8741],\n",
      "         [ 6.1670, -3.5847, -4.2814,  ..., -2.9591, -3.5548, -3.8621]],\n",
      "\n",
      "        [[ 5.9147, -3.5535, -4.3209,  ..., -2.9299, -3.5479, -3.9010],\n",
      "         [ 6.0345, -3.5448, -4.3420,  ..., -3.0742, -3.5281, -3.8356],\n",
      "         [ 5.9793, -3.5165, -4.3625,  ..., -2.9275, -3.5005, -3.8932],\n",
      "         ...,\n",
      "         [ 5.9388, -3.5869, -4.1603,  ..., -2.9494, -3.5491, -3.8583],\n",
      "         [ 5.9922, -3.5738, -4.3495,  ..., -2.9511, -3.5716, -3.8752],\n",
      "         [ 6.1687, -3.5785, -4.2824,  ..., -2.9769, -3.5389, -3.8586]]],\n",
      "       grad_fn=<AddBackward0>) \n",
      "********Done*******\n",
      "\n",
      "\n",
      "argmax:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "view adjusted: \n",
      " tensor([[ 6.0307, -3.5627, -4.3091,  ..., -2.9609, -3.6032, -3.8193],\n",
      "        [ 6.0661, -3.5324, -4.3047,  ..., -3.0001, -3.5479, -3.8325],\n",
      "        [ 6.0088, -3.5212, -4.3316,  ..., -2.8538, -3.5209, -3.8411],\n",
      "        ...,\n",
      "        [ 5.9388, -3.5869, -4.1603,  ..., -2.9494, -3.5491, -3.8583],\n",
      "        [ 5.9922, -3.5738, -4.3495,  ..., -2.9511, -3.5716, -3.8752],\n",
      "        [ 6.1687, -3.5785, -4.2824,  ..., -2.9769, -3.5389, -3.8586]],\n",
      "       grad_fn=<ViewBackward>) \n",
      "*******Done********\n",
      "view-adjusted-shape:  torch.Size([6272, 14744])\n"
     ]
    }
   ],
   "source": [
    "## code to view text\n",
    "\n",
    "from einops import rearrange\n",
    "for i,batch in enumerate(train_iter):\n",
    "    if i == 1:\n",
    "        break\n",
    "    src = batch.text\n",
    "    trg = batch.summ\n",
    "    trg_inp, trg_out = trg[:-1, :], trg[1:, :]\n",
    "    \n",
    "    \n",
    "    print(\"text: \",[src_list[i] for i in src.squeeze(1).transpose(0,1)[1].tolist()])\n",
    "    \n",
    "    print(\"\\n\\nsumm: \",[src_list[i] for i in trg.squeeze(1).transpose(0,1)[1].tolist()])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    memory = model.transformer.encoder(model.pos_enc(model.embed_src(src) * math.sqrt(model.d_model)))\n",
    "        \n",
    "    out = model.fc(model.transformer.decoder(model.pos_enc(model.embed_tgt(trg) * math.sqrt(model.d_model)), memory))\n",
    "\n",
    "    \n",
    "    out_  = rearrange(out,'t b e -> b t e')\n",
    "    \n",
    "    print(\"out:\\n\",out,\"\\n********Done*******\")\n",
    "    \n",
    "    print(\"\\n\\nargmax: \",out_.argmax(2)[0].tolist())\n",
    "    \n",
    "    l = out_.argmax(2)[1].tolist()\n",
    "    \n",
    "    print([src_list[i] for i in l])\n",
    "    \n",
    "    output_dim = out.shape[-1]\n",
    "    \n",
    "    print(\"view adjusted: \\n\",out.view(-1,output_dim),\"\\n*******Done********\")\n",
    "    \n",
    "    print(\"view-adjusted-shape: \",out.view(-1,output_dim).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2idx_mapper(text,vocab_dict):\n",
    "    text = '<sos> ' + text + ' <eos>'\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    tokens = tokenizer(text)\n",
    "#     print(tokens)\n",
    "    ids = [vocab_dict[i] for i in tokens]\n",
    "    return ids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2seq(idx_list,vocab_list):\n",
    "    return \"\".join(vocab_list[i] for i in idx_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text,model,vocab_list,vocab_dict,max_len=50,device='cpu'):\n",
    "    src = torch.Tensor(word2idx_mapper(text,vocab_dict)).long().unsqueeze(1).to(device)\n",
    "    \n",
    "    memory = model.transformer.encoder(model.pos_enc(model.embed_src(src) * math.sqrt(model.d_model)))\n",
    "    out_idx = [vocab_dict['<sos>'],]\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        trg = torch.LongTensor(out_idx).unsqueeze(1).to(device)\n",
    "        \n",
    "        output = model.fc(model.transformer.decoder(model.pos_enc(model.embed_tgt(trg) * math.sqrt(model.d_model)), memory))\n",
    "        \n",
    "        \n",
    "        out_token = output.argmax(2)[-1].item()\n",
    "        \n",
    "        out_idx.append(out_token)\n",
    "        \n",
    "    return idx2seq(out_idx,vocab_list)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', 'large', 'groups', 'of', 'people', 'have', 'gathered', 'in', 'the', 'stadium', 'to', 'witness', 'the', 'game', '<eos>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 607, 0, 0, 27, 8, 2661, 0, 0, 1576, 0, 686, 0, 253, 3]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx_mapper(\"Large groups of people have gathered in the stadium to witness the game\",src_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_summary(\"Large groups of people have gathered in the stadium to witness the game\",model,src_list,src_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
