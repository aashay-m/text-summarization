{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerDecoder,TransformerDecoderLayer\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.nn import Transformer\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mapka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from functools import lru_cache\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.utils as utils\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "cached_lemmatize = lru_cache(maxsize=50000)(WordNetLemmatizer().lemmatize)\n",
    "from gensim.utils import simple_preprocess, to_unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_dim,emb_dim,enc_hid_dim,dec_hid_dim,dropout=0.5):\n",
    "        \n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim,emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        \n",
    "        self.fc = nn.Linear( enc_hid_dim * 2, dec_hid_dim )\n",
    "        \n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(X))\n",
    "        \n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        \n",
    "        hidden = F.tanh( self.fc ( torch.cat( (hidden[-2,:,:], hidden[-1, : , : ] ), dim = 1 ) ) )\n",
    "        \n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"data\"\n",
    "train_file_X = os.path.join(base_dir,\"train.source\")\n",
    "train_file_y = os.path.join(base_dir,\"train.target\")\n",
    "test_file_X = os.path.join(base_dir,\"test.source\")\n",
    "test_file_y = os.path.join(base_dir,\"test.target\")\n",
    "val_file_X = os.path.join(base_dir,\"val.source\")\n",
    "val_file_y = os.path.join(base_dir,\"val.target\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "STOP_WORDS = [\"i\", \"a\", \"about\", \"an\", \"are\", \"as\", \"at\", \"be\", \"by\", \n",
    "                \"for\", \"from\", \"how\", \"in\", \"is\", \"it\", \"of\", \"on\", \"or\", \"that\", \"the\", \n",
    "                \"this\", \"to\", \"was\", \"what\", \"when\", \"where\", \"who\", \"will\", \"with\"]\n",
    "\n",
    "def ExpandContractions(contraction):\n",
    "\n",
    "    contraction = re.sub(r\"won\\'t\", \"will not\", contraction)\n",
    "    contraction = re.sub(r\"can\\'t\", \"can not\", contraction)\n",
    "\n",
    "    contraction = re.sub(r\"n\\'t\", \" not\", contraction)\n",
    "    contraction = re.sub(r\"\\'re\", \" are\", contraction)\n",
    "    contraction = re.sub(r\"\\'s\", \" is\", contraction)\n",
    "    contraction = re.sub(r\"\\'d\", \" would\", contraction)\n",
    "    contraction = re.sub(r\"\\'ll\", \" will\", contraction)\n",
    "    contraction = re.sub(r\"\\'t\", \" not\", contraction)\n",
    "    contraction = re.sub(r\"\\'ve\", \" have\", contraction)\n",
    "    contraction = re.sub(r\"\\'m\", \" am\", contraction)\n",
    "\n",
    "    return contraction\n",
    "\n",
    "def PreProcess(line):\n",
    "    \n",
    "    line = line.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    line = ExpandContractions(line)\n",
    "    line = simple_preprocess(to_unicode(line))\n",
    "    line = [cached_lemmatize(word) for word in line if word not in STOP_WORDS]\n",
    "\n",
    "    line = \" \".join(line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineSentenceGenerator(object):\n",
    "\n",
    "    def __init__(self, source, preprocess=None, max_sentence_length=4000, limit=None, preprocess_flag=True):\n",
    "        self.source = source\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.limit = limit\n",
    "        self.input_files = []\n",
    "\n",
    "        if preprocess != None and callable(preprocess) and preprocess_flag:\n",
    "            self.preprocess = preprocess\n",
    "        else:\n",
    "            self.preprocess = lambda line: line.rstrip(\"\\r\\n\")\n",
    "\n",
    "        if isinstance(self.source, list):\n",
    "            print('List of files given as source. Verifying entries and using.')\n",
    "            self.input_files = [filename for filename in self.source if os.path.isfile(filename)]\n",
    "            self.input_files.sort()  # makes sure it happens in filename order\n",
    "\n",
    "        elif os.path.isfile(self.source):\n",
    "            print('Single file given as source, rather than a list of files. Wrapping in list.')\n",
    "            self.input_files = [self.source]  # force code compatibility with list of files\n",
    "\n",
    "        elif os.path.isdir(self.source):\n",
    "            self.source = os.path.join(self.source, '')  # ensures os-specific slash at end of path\n",
    "            print('Directory of files given as source. Reading directory %s', self.source)\n",
    "            self.input_files = os.listdir(self.source)\n",
    "            self.input_files = [self.source + filename for filename in self.input_files]  # make full paths\n",
    "            self.input_files.sort()  # makes sure it happens in filename order\n",
    "        else:  # not a file or a directory, then we can't do anything with it\n",
    "            raise ValueError('Input is neither a file nor a path nor a list')\n",
    "        print('Files read into LineSentenceGenerator: %s' % ('\\n'.join(self.input_files)))\n",
    "\n",
    "        self.token_count = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        for file_name in self.input_files:\n",
    "            print('Reading file %s', file_name)\n",
    "            with open(file_name, 'rb') as fin:\n",
    "                for line in itertools.islice(fin, self.limit):\n",
    "                    line = self.preprocess(utils.to_unicode(line))\n",
    "                    self.token_count += len(line)\n",
    "                    i = 0\n",
    "                    while i < len(line):\n",
    "                        yield line[i:i + self.max_sentence_length]\n",
    "                        i += self.max_sentence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.token_count > 0:\n",
    "            return self.token_count\n",
    "        else:\n",
    "            return len(self.input_files)\n",
    "\n",
    "    def __bool__(self):\n",
    "        return self.has_data()\n",
    "\n",
    "    def is_empty(self):\n",
    "        return len(self.input_files) == 0\n",
    "\n",
    "    def has_data(self):\n",
    "        return not self.is_empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Dataset,Example\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "SRC = Field(tokenize = get_tokenizer(\"spacy\"),\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = False)\n",
    "\n",
    "# TRG = Field(tokenize = get_tokenizer(\"basic_english\"),\n",
    "#             init_token = '<sos>',\n",
    "#             eos_token = '<eos>',\n",
    "#             is_target = True,\n",
    "#             lower = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(X,y,limit=1000):\n",
    "    examples = []\n",
    "    fields = {'text-tokens': ('text', SRC),\n",
    "              'summ-tokens': ('summ', SRC)}\n",
    "    for i,(x,y) in enumerate(zip(LineSentenceGenerator(X,PreProcess),LineSentenceGenerator(y,PreProcess))):\n",
    "        if i > limit:\n",
    "            break\n",
    "        text_field =x \n",
    "        summ_field = y \n",
    "       \n",
    "        e = Example.fromdict({\"text-tokens\": text_field, \"summ-tokens\": summ_field},\n",
    "                             fields=fields)\n",
    "        examples.append(e)\n",
    "    print(\"examples: \\n\", examples[0])\n",
    "    return Dataset(examples, fields=[('text', SRC), ('summ', SRC)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single file given as source, rather than a list of files. Wrapping in list.\n",
      "Files read into LineSentenceGenerator: data\\train.source\n",
      "Single file given as source, rather than a list of files. Wrapping in list.\n",
      "Files read into LineSentenceGenerator: data\\train.target\n",
      "Reading file %s data\\train.source\n",
      "Reading file %s data\\train.target\n",
      "examples: \n",
      " <torchtext.data.example.Example object at 0x00000282089EB7C8>\n",
      "Single file given as source, rather than a list of files. Wrapping in list.\n",
      "Files read into LineSentenceGenerator: data\\test.source\n",
      "Single file given as source, rather than a list of files. Wrapping in list.\n",
      "Files read into LineSentenceGenerator: data\\test.target\n",
      "Reading file %s data\\test.source\n",
      "Reading file %s data\\test.target\n",
      "examples: \n",
      " <torchtext.data.example.Example object at 0x0000028219345B48>\n",
      "Single file given as source, rather than a list of files. Wrapping in list.\n",
      "Files read into LineSentenceGenerator: data\\val.source\n",
      "Single file given as source, rather than a list of files. Wrapping in list.\n",
      "Files read into LineSentenceGenerator: data\\val.target\n",
      "Reading file %s data\\val.source\n",
      "Reading file %s data\\val.target\n",
      "examples: \n",
      " <torchtext.data.example.Example object at 0x00000282171C8FC8>\n"
     ]
    }
   ],
   "source": [
    "train_data = read_data(train_file_X,train_file_y,1000)\n",
    "test_data = read_data(test_file_X,test_file_y,200)\n",
    "val_data = read_data(val_file_X,val_file_y,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  ['editor', 'note', 'our', 'behind', 'scene', 'series', 'cnn', 'correspondent', 'share', 'their', 'experience', 'covering', 'news', 'and', 'analyze', 'story', 'behind', 'event', 'here', 'soledad', 'obrien', 'take', 'user', 'inside', 'jail', 'many', 'inmate', 'mentally', 'ill', 'inmate', 'housed', 'forgotten', 'floor', 'many', 'mentally', 'ill', 'inmate', 'housed', 'miami', 'before', 'trial', 'miami', 'florida', 'cnn', 'ninth', 'floor', 'miamidade', 'pretrial', 'detention', 'facility', 'dubbed', 'forgotten', 'floor', 'here', 'inmate', 'most', 'severe', 'mental', 'illness', 'incarcerated', 'until', 'they', 're', 'ready', 'appear', 'court', 'most', 'often', 'they', 'face', 'drug', 'charge', 'charge', 'assaulting', 'officer', 'charge', 'judge', 'steven', 'leifman', 'say', 'usually', 'avoidable', 'felony', 'he', 'say', 'arrest', 'often', 'result', 'confrontation', 'police', 'mentally', 'ill', 'people', 'often', 'wo', 'nt', 'do', 'they', 're', 'told', 'police', 'arrive', 'scene', 'confrontation', 'seems', 'exacerbate', 'their', 'illness', 'and', 'they', 'become', 'more', 'paranoid', 'delusional', 'and', 'le', 'likely', 'follow', 'direction', 'according', 'leifman', 'so', 'they', 'end', 'up', 'ninth', 'floor', 'severely', 'mentally', 'disturbed', 'but', 'not', 'getting', 'any', 'real', 'help', 'because', 'they', 're', 'jail', 'we', 'toured', 'jail', 'leifman', 'he', 'well', 'known', 'miami', 'advocate', 'justice', 'and', 'mentally', 'ill', 'even', 'though', 'we', 'were', 'not', 'exactly', 'welcomed', 'open', 'arm', 'guard', 'we', 'were', 'given', 'permission', 'shoot', 'videotape', 'and', 'tour', 'floor', 'go', 'inside', 'forgotten', 'floor', 'first', 'it', 'hard', 'determine', 'people', 'prisoner', 'wearing', 'sleeveless', 'robe', 'imagine', 'cutting', 'hole', 'arm', 'and', 'foot', 'heavy', 'wool', 'sleeping', 'bag', 'that', 's', 'kind', 'they', 'look', 'like', 'they', 're', 'designed', 'keep', 'mentally', 'ill', 'patient', 'injuring', 'themselves', 'that', 's', 'also', 'why', 'they', 'have', 'no', 'shoe', 'lace', 'mattress', 'leifman', 'say', 'onethird', 'all', 'people', 'miamidade', 'county', 'jail', 'mentally', 'ill', 'so', 'he', 'say', 'sheer', 'volume', 'overwhelming', 'system', 'and', 'result', 'we', 'see', 'ninth', 'floor', 'course', 'jail', 'so', 'it', 'not', 'supposed', 'warm', 'and', 'comforting', 'but', 'light', 'glare', 'cell', 'tiny', 'and', 'it', 'loud', 'we', 'see', 'two', 'sometimes', 'three', 'men', 'sometimes', 'robe', 'sometimes', 'naked', 'lying', 'sitting', 'their', 'cell', 'am', 'son', 'president', 'you', 'need', 'get', 'me', 'out', 'here', 'one', 'man', 'shout', 'me', 'he', 'absolutely', 'serious', 'convinced', 'help', 'way', 'if', 'only', 'he', 'could', 'reach', 'white', 'house', 'leifman', 'tell', 'me', 'these', 'often', 'circulate', 'through', 'system', 'occasionally', 'stabilizing', 'mental', 'hospital', 'only', 'return', 'jail', 'face', 'their', 'charge', 'it', 'brutally', 'unjust', 'his', 'mind', 'and', 'he', 'ha', 'become', 'strong', 'advocate', 'changing', 'thing', 'miami', 'over', 'meal', 'later', 'we', 'talk', 'thing', 'got', 'way', 'mental', 'patient', 'leifman', 'say', 'year', 'ago', 'people', 'were', 'considered', 'lunatic', 'and', 'they', 'were', 'locked', 'up', 'jail', 'even', 'if', 'they', 'had', 'no', 'charge', 'against', 'them', 'they', 'were', 'just', 'considered', 'unfit', 'society', 'over', 'year', 'he', 'say', 'there', 'some', 'public', 'outcry', 'and', 'mentally', 'ill', 'were', 'moved', 'out', 'jail', 'and', 'into', 'hospital', 'but', 'leifman', 'say', 'many', 'these', 'mental', 'hospital', 'were', 'so', 'horrible', 'they', 'were', 'shut', 'down', 'did', 'patient', 'go', 'nowhere', 'street', 'they', 'became', 'many', 'case', 'homeless', 'he', 'say', 'they', 'never', 'got', 'treatment', 'leifman', 'say', 'there', 'were', 'more', 'than', 'half', 'million', 'people', 'state', 'mental', 'hospital', 'and', 'today', 'number', 'ha', 'been', 'reduced', 'percent', 'and', 'people', 'mental', 'hospital', 'judge', 'say', 'he', 'working', 'change', 'starting', 'many', 'inmate', 'would', 'otherwise', 'have', 'been', 'brought', 'forgotten', 'floor', 'instead', 'sent', 'new', 'mental', 'health', 'facility', 'first', 'step', 'journey', 'toward', 'longterm', 'treatment', 'not', 'just', 'punishment', 'leifman', 'say', 'it', 'not', 'complete', 'answer', 'but', 'it', 'start', 'leifman', 'say', 'best', 'part', 'it', 'winwin', 'solution', 'patient', 'win', 'family', 'relieved', 'and', 'state', 'save', 'money', 'simply', 'not', 'cycling', 'these', 'prisoner', 'through', 'again', 'and', 'again', 'and', 'leifman', 'justice', 'served', 'email', 'friend']\n",
      "\n",
      "text-len:  510\n",
      "\n",
      "\n",
      "summary:  ['mentally', 'ill', 'inmate', 'miami', 'housed', 'forgotten', 'floor', 'judge', 'steven', 'leifman', 'say', 'most', 'there', 'result', 'avoidable', 'felony', 'while', 'cnn', 'tour', 'facility', 'patient', 'shout', 'am', 'son', 'president', 'leifman', 'say', 'system', 'unjust', 'and', 'he', 'fighting', 'change']\n"
     ]
    }
   ],
   "source": [
    "print(\"text: \",train_data[0].text)\n",
    "print(\"\\ntext-len: \",len(train_data[0].text))\n",
    "print(\"\\n\\nsummary: \",train_data[0].summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': <torchtext.data.field.Field at 0x282089830c8>,\n",
       " 'summ': <torchtext.data.field.Field at 0x282089830c8>}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  ['cnna', 'frenchlanguage', 'global', 'television', 'network', 'regained', 'control', 'one', 'it', 'channel', 'thursday', 'after', 'cyberattack', 'day', 'earlier', 'crippled', 'it', 'broadcast', 'and', 'social', 'medium', 'account', 'television', 'network', 'tv', 'monde', 'gradually', 'regaining', 'control', 'it', 'channel', 'and', 'social', 'medium', 'outlet', 'after', 'suffering', 'network', 'director', 'called', 'extremely', 'powerful', 'cyberattack', 'addition', 'it', 'channel', 'tv', 'monde', 'lost', 'control', 'it', 'social', 'medium', 'outlet', 'and', 'it', 'website', 'director', 'yves', 'bigot', 'said', 'video', 'message', 'posted', 'later', 'facebook', 'mobile', 'site', 'which', 'still', 'active', 'network', 'said', 'hacked', 'islamist', 'group', 'isi', 'logo', 'and', 'marking', 'appeared', 'tv', 'monde', 'social', 'medium', 'account', 'but', 'there', 'no', 'immediate', 'claim', 'responsibility', 'isi', 'any', 'other', 'group', 'day', 'broke', 'thursday', 'europe', 'network', 'had', 'regained', 'use', 'one', 'it', 'channel', 'and', 'it', 'facebook', 'page', 'paul', 'germain', 'chain', 'editor', 'chief', 'told', 'bfmtv', 'cnn', 'affiliate', 'france', 'however', 'late', 'morning', 'number', 'page', 'network', 'website', 'had', 'message', 'saying', 'they', 'were', 'under', 'maintenance', 'outage', 'began', 'around', 'pm', 'paris', 'time', 'pm', 'et', 'wednesday', 'tv', 'monde', 'offer', 'roundtheclock', 'entertainment', 'and', 'news', 'programming', 'reach', 'million', 'home', 'worldwide', 'according', 'ministry', 'culture', 'and', 'communication', 'function', 'under', 'partnership', 'among', 'government', 'france', 'canada', 'and', 'switzerland', 'well', 'federation', 'other', 'network', 'provide', 'content', 'tv', 'monde', 'include', 'cnn', 'affiliate', 'france', 'and', 'france', 'france', 'and', 'radio', 'france', 'international']\n",
      "\n",
      "\n",
      "summ:  ['don', 'mcleans', 'american', 'pie', 'lyric', 'auctioned', 'million', 'song', 'dense', 'symbolism', 'mclean', 'say', 'lyric', 'note', 'reveal', 'meaning', 'pie', 'mcleans', 'biggest', 'hit', 'no']\n"
     ]
    }
   ],
   "source": [
    "print(\"text: \", test_data[100].text)\n",
    "print(\"\\n\\nsumm: \",test_data[100].summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data.text, min_freq = 2)\n",
    "# TRG.build_vocab(train_data.summ, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14744"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SRC.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iter = BucketIterator(train_data,BATCH_SIZE, shuffle=True,\n",
    "                                                 sort_key=lambda x: len(x.text), sort_within_batch=True)\n",
    "\n",
    "val_iter = BucketIterator(val_data, BATCH_SIZE, sort_key=lambda x: len(x.text), sort_within_batch=True)\n",
    "test_iter = BucketIterator(test_data,BATCH_SIZE, sort_key=lambda x: len(x.text), sort_within_batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([450, 128]) \n",
      "\n",
      "\n",
      "tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
      "        [2825,  313,  252,  ...,   22,   22,   22],\n",
      "        [ 335,  436,   22,  ..., 5436,   85, 5613],\n",
      "        ...,\n",
      "        [   6,   42,    1,  ...,    1,    1,    1],\n",
      "        [   5,    3,    1,  ...,    1,    1,    1],\n",
      "        [   3,    1,    1,  ...,    1,    1,    1]])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'src_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-e2dc3979f184>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#     print(batch.text[:-1,:].shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"text: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msrc_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msumm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nsumm: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msrc_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-e2dc3979f184>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#     print(batch.text[:-1,:].shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"text: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msrc_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msumm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nsumm: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msrc_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'src_list' is not defined"
     ]
    }
   ],
   "source": [
    "for batch in train_iter:\n",
    "    print(batch.text.shape,\"\\n\\n\")\n",
    "    x = batch.text\n",
    "    print(x)\n",
    "#     print(batch.text[:-1,:].shape)\n",
    "    print(\"text: \",[src_list[i] for i in x.squeeze(1).transpose(0,1)[0].tolist()])\n",
    "    y = batch.summ\n",
    "    print(\"\\nsumm: \",[src_list[i] for i in y.squeeze(1).transpose(0,1)[0].tolist()])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class TransformerSummarizer(nn.Module):\n",
    "    def __init__(self, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length,vocab_size, pad_idx,  d_model=None, pos_dropout =0.1, trans_dropout= 0.1,embeddings=None):\n",
    "        super().__init__()\n",
    "       \n",
    "        if embeddings is None:\n",
    "            self.embed_src = nn.Embedding(vocab_size, d_model)\n",
    "            self.embed_tgt = nn.Embedding(vocab_size, d_model)\n",
    "        else:\n",
    "            d_model = embeddings.size(1)\n",
    "            self.d_model = embeddings.size(1)\n",
    "            self.embed_src = nn.Embedding(*embeddings.shape)\n",
    "            self.embed_src.weight = nn.Parameter(embeddings,requires_grad=False)\n",
    "            \n",
    "            self.embed_tgt = nn.Embedding(*embeddings.shape)\n",
    "            self.embed_tgt.weight = nn.Parameter(embeddings,requires_grad=False)\n",
    "        \n",
    "        \n",
    "        self.pos_enc = PositionalEncoding(d_model, pos_dropout, max_seq_length)\n",
    "\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, trans_dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        self.src_mask = None\n",
    "        self.tgt_mask = None\n",
    "        self.memory_mask = None\n",
    "        \n",
    "    def generate_square_mask(self,sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    def make_pad_mask(self,seq,pad_idx):\n",
    "        mask = (seq == pad_idx).transpose(0,1)\n",
    "        return mask\n",
    "    \n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        if self.tgt_mask is None or self.tgt_mask.size(0) != len(trg):\n",
    "            self.trg_mask = self.generate_square_mask(len(trg)).to(trg.device)\n",
    "        \n",
    "#         print(\"Before Embed: \",src.shape,tgt.shape,sep=\"\\n\")\n",
    "        \n",
    "        src_pad_mask = self.make_pad_mask(src,self.pad_idx)\n",
    "        tgt_pad_mask = self.make_pad_mask(tgt,self.pad_idx)\n",
    "        \n",
    "        print(\"src_pad_mask: \",src_pad_mask,\"\\n *****DONE****\")\n",
    "        print(\"trg_pad_mask: \",tgt_pad_mask,\"\\n *****DONE****\")\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        src = self.pos_enc(self.embed_src(src) * math.sqrt(self.d_model))\n",
    "\n",
    "        tgt = self.pos_enc(self.embed_tgt(tgt) * math.sqrt(self.d_model))\n",
    "        print(tgt.shape)\n",
    "        \n",
    "\n",
    "        output = self.transformer(src, tgt, src_mask=self.src_mask, tgt_mask=self.tgt_mask, memory_mask=self.memory_mask, \n",
    "                                 src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask, memory_key_padding_mask=src_pad_mask)\n",
    "        \n",
    "        return self.fc(output)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TGT_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14744\n"
     ]
    }
   ],
   "source": [
    "# trg = len(TRG.vocab)\n",
    "# EMB_DIM = 200\n",
    "SEQ_LEN = 4000\n",
    "\n",
    "D_MODEL = 200 #embedding_size\n",
    "DIM_FEEDFORWARD = 300\n",
    "VOCAB_SIZE = len(SRC.vocab)\n",
    "print(VOCAB_SIZE)\n",
    "ATTENTION_HEADS = 6\n",
    "N_LAYERS = 1\n",
    "\n",
    "\n",
    "\n",
    "# vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length, pos_dropout, trans_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import FastText\n",
    "\n",
    "ff = FastText(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = ff.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings =  ff.get_vecs_by_tokens(SRC.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14744, 300])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerSummarizer( ATTENTION_HEADS,N_LAYERS, N_LAYERS, DIM_FEEDFORWARD, SEQ_LEN,VOCAB_SIZE,PAD_IDX,embeddings=embeddings).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerSummarizer(\n",
       "  (embed_src): Embedding(14744, 300)\n",
       "  (embed_tgt): Embedding(14744, 300)\n",
       "  (pos_enc): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=300, out_features=14744, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "def train(model: nn.Module,\n",
    "          iterator: BucketIterator,\n",
    "          num_batches: int,\n",
    "          optimizer: optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float):\n",
    "    \n",
    "    print(\"Training......\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in tqdm(iterator,total=num_batches):\n",
    "        \n",
    "#         if i == 1:\n",
    "#             break\n",
    "\n",
    "        src = batch.text\n",
    "        trg = batch.summ\n",
    "        \n",
    "#         tgt_inp, tgt_out = tgt[:, :-1], tgt[:, 1:]\n",
    "#         tgt_mask = gen_nopeek_mask(tgt_inp.shape[1]).to('cuda')\n",
    "\n",
    "#         trg_inp = trg[:,:-1] \n",
    "\n",
    "        trg_inp, trg_out = trg[:-1, :], trg[1:, :]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src.to(device), trg_inp.to(device))\n",
    "    \n",
    "        output = output.view(-1, output.shape[-1])\n",
    "\n",
    "        loss = criterion(output, trg_out.view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    print(\"Training Done.....\")\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module,\n",
    "             iterator: BucketIterator,\n",
    "             num_batches:int,\n",
    "             criterion: nn.Module):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    print(\"Evaluating....\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in tqdm(enumerate(iterator),total=num_batches):\n",
    "            \n",
    "            if i == 1:\n",
    "                break\n",
    "            src = batch.text\n",
    "            trg = batch.summ\n",
    "        \n",
    "#             trg_inp, trg_out = trg[:-1, :], trg[1:, :]\n",
    "\n",
    "            output = model(src.to(device), trg_inp.to(device))\n",
    "\n",
    "            output = output.view(-1,output.shape[-1])\n",
    "\n",
    "            loss = criterion(output, trg[1:,:].view(-1))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        print(\"Evaluating Done........\")\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_list = SRC.vocab.itos\n",
    "src_dict = SRC.vocab.stoi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([215, 128])\n",
      "torch.Size([46, 128])\n",
      "text:  ['<sos>', 'london', 'england', 'milan', 'goalkeeper', 'dida', 'ha', 'been', 'cleared', 'play', 'next', 'month', 'champion', 'league', 'match', 'shakhtar', 'donetsk', 'after', 'partially', 'winning', 'his', 'appeal', 'uefa', 'against', 'twomatch', 'ban', 'dida', 'ha', 'had', 'one', 'game', 'his', 'twomatch', 'ban', 'suspended', 'year', 'following', 'appeal', 'uefa', 'brazilian', 'dida', 'also', 'fined', 'swiss', 'franc', 'european', 'football', 'ruling', 'body', 'following', 'incident', 'involving', 'supporter', 'during', 'champion', 'clash', 'against', 'celtic', 'scotland', 'october', 'yearold', 'brazilian', 'initially', 'banned', 'two', 'game', 'his', '<unk>', 'following', 'celtic', 'fan', 'encroachment', 'onto', 'pitch', 'during', 'defeat', 'celtic', 'park', 'following', 'monday', 'appeal', 'hearing', 'dida', 'suspended', 'two', 'match', 'but', 'one', 'match', 'now', 'deferred', '<unk>', 'period', 'one', 'year', 'said', '<unk>', 'uefas', 'web', 'site', 'dida', 'sits', 'out', 'home', 'tie', 'against', 'shakhtar', 'wednesday', 'after', 'inquiry', 'based', 'article', 'paragraph', 'uefa', 'disciplinary', 'regulation', 'principle', 'conduct', 'under', 'which', 'member', 'association', 'club', 'well', 'their', 'player', 'official', 'and', 'member', 'shall', 'conduct', 'themselves', 'according', 'principle', 'loyalty', 'integrity', 'and', 'sportsmanship', 'however', 'dida', 'only', 'serve', 'second', 'match', 'his', 'ban', 'if', 'he', 'commits', 'similar', 'offense', 'theatrical', 'overreaction', 'during', 'next', 'month', 'freeing', 'him', 'trip', 'ukraine', 'uefa', 'said', 'their', 'appeal', 'body', 'took', 'note', 'dida', 'expressed', 'his', 'sincere', 'regret', 'regard', 'his', 'conduct', 'during', 'match', 'ac', 'milan', '<unk>', '<unk>', 'galliani', 'satisfied', 'uefas', 'decision', 'believe', 'fair', 'against', '<unk>', 'suspension', 'but', 'one', 'seems', 'ok', 'dida', 'ha', 'made', 'mistake', 'but', 'his', 'error', 'did', 'not', '<unk>', 'anyone', 'not', 'celtic', 'nor', 'any', 'player', 'said', 'galliani', 'email', 'friend', '<eos>']\n",
      "\n",
      "\n",
      "summ:  ['<sos>', 'chavez', 'say', 'uribe', 'had', 'asked', 'him', 'help', 'secure', 'release', 'hostage', 'uribe', 'cited', 'chavez', 'direct', 'communication', '<unk>', 'top', 'general', 'breach', 'chavez', 'accuses', 'uribe', 'lying', 'chavez', 'say', 'his', 'arm', 'open', 'colombian', 'people', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "trg_inp:  torch.Size([45, 128])\n",
      "trg_out:  torch.Size([45, 128])\n",
      "out:\n",
      " tensor([[[-1.0827e-01,  2.1935e-01, -1.5646e+00,  ...,  2.5770e-01,\n",
      "           4.5229e-01, -5.5483e-01],\n",
      "         [-3.6066e-01,  5.1077e-01, -8.9085e-01,  ...,  3.4908e-01,\n",
      "           9.4618e-01,  1.5037e-01],\n",
      "         [-3.2803e-01, -6.3468e-01, -1.2484e+00,  ...,  2.3193e-01,\n",
      "           5.8125e-01, -4.9382e-01],\n",
      "         ...,\n",
      "         [ 3.2613e-01, -5.2085e-01, -5.7918e-01,  ...,  3.9841e-01,\n",
      "           1.4720e-01, -5.3972e-01],\n",
      "         [-7.0673e-01, -1.8212e-01, -3.3248e-01,  ...,  4.5113e-02,\n",
      "           2.4338e-02, -1.6446e-01],\n",
      "         [-2.3438e-01,  5.2699e-01, -5.2004e-01,  ..., -7.5956e-02,\n",
      "          -1.7437e-01, -1.1967e-01]],\n",
      "\n",
      "        [[ 1.2183e+00, -4.0337e-01, -1.1089e+00,  ...,  4.2791e-01,\n",
      "           1.1745e-01, -5.3143e-01],\n",
      "         [-2.0355e-01, -4.5389e-01, -4.6176e-01,  ..., -5.3164e-01,\n",
      "           4.6407e-01, -8.6235e-01],\n",
      "         [-1.7886e-01, -8.9373e-01, -1.7613e+00,  ..., -2.2092e-01,\n",
      "           9.9958e-01,  1.0623e+00],\n",
      "         ...,\n",
      "         [ 4.5723e-02, -1.1875e-01, -8.6056e-01,  ..., -4.6638e-01,\n",
      "           1.3529e+00,  2.4381e-01],\n",
      "         [-1.4282e-01,  2.0058e-01, -9.9965e-01,  ...,  1.2776e-01,\n",
      "          -1.8069e-01, -1.2140e-01],\n",
      "         [-1.5808e-02, -2.4391e-01, -1.0648e+00,  ...,  5.0471e-01,\n",
      "           2.7566e-01, -7.4054e-02]],\n",
      "\n",
      "        [[ 4.3090e-02, -8.5481e-01, -1.0291e+00,  ...,  3.9811e-01,\n",
      "          -3.1260e-01, -1.4497e-01],\n",
      "         [ 1.4915e-01,  2.5815e-01, -8.3659e-01,  ...,  2.5058e-01,\n",
      "           2.0181e-01, -7.1505e-02],\n",
      "         [ 5.5806e-01,  3.7170e-02, -9.2646e-02,  ...,  3.9832e-01,\n",
      "           3.2818e-01,  1.4380e-01],\n",
      "         ...,\n",
      "         [ 6.4462e-01,  3.7609e-01, -8.8032e-01,  ...,  5.2141e-01,\n",
      "          -2.4470e-01, -5.3943e-01],\n",
      "         [-2.2296e-01,  7.1194e-01, -8.2325e-01,  ..., -2.7121e-01,\n",
      "           4.1678e-02,  1.0156e-01],\n",
      "         [-4.7250e-01,  1.4986e-01, -1.7162e-01,  ...,  6.8059e-01,\n",
      "           1.0889e+00,  6.0923e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 5.8032e-01,  1.5987e-01, -1.2336e+00,  ..., -6.0191e-01,\n",
      "           3.4720e-01, -4.6260e-01],\n",
      "         [ 3.5050e-01,  2.7960e-01, -6.5225e-01,  ...,  4.4534e-01,\n",
      "           3.7668e-01, -1.0834e+00],\n",
      "         [ 4.3187e-01,  1.3834e-01, -1.1921e+00,  ...,  3.6589e-01,\n",
      "           7.9171e-01, -5.8053e-01],\n",
      "         ...,\n",
      "         [ 1.0711e-01,  3.0242e-01, -4.2177e-01,  ...,  1.2964e-02,\n",
      "           2.3526e-01, -9.4376e-01],\n",
      "         [-7.0995e-02, -2.7767e-02, -8.2902e-01,  ..., -9.9687e-02,\n",
      "           1.9912e-01, -6.2295e-01],\n",
      "         [-5.9811e-01,  4.5229e-01, -2.7462e-01,  ...,  3.4277e-01,\n",
      "          -1.1430e-01, -1.0010e+00]],\n",
      "\n",
      "        [[ 2.4815e-01,  5.6210e-01, -1.6066e+00,  ...,  2.3381e-01,\n",
      "           9.2457e-01, -8.2684e-01],\n",
      "         [ 1.0116e-01,  5.8326e-01, -1.0165e+00,  ...,  8.1686e-01,\n",
      "           4.5460e-02, -1.1689e+00],\n",
      "         [ 1.7803e-01,  9.1759e-02, -1.0466e+00,  ...,  4.9232e-01,\n",
      "           5.1272e-01, -1.0893e+00],\n",
      "         ...,\n",
      "         [ 5.8068e-02,  4.9768e-01, -4.0765e-01,  ...,  4.1692e-01,\n",
      "           5.0082e-02, -3.5079e-01],\n",
      "         [-8.9453e-01, -1.5783e-01, -5.0944e-01,  ...,  1.6313e-02,\n",
      "          -1.9569e-01, -4.1734e-01],\n",
      "         [-2.7347e-01,  5.6682e-01, -7.5123e-01,  ...,  1.2664e-01,\n",
      "          -1.7305e-01, -8.0635e-01]],\n",
      "\n",
      "        [[-1.8502e-01,  1.6857e-01, -2.4427e-01,  ..., -1.3774e-01,\n",
      "           4.7003e-01, -1.1134e+00],\n",
      "         [ 3.2547e-01,  5.5922e-01,  1.7627e-01,  ...,  1.0941e+00,\n",
      "           6.8574e-01, -9.9846e-01],\n",
      "         [ 2.5722e-01,  2.0805e-01, -1.5726e+00,  ...,  7.1466e-01,\n",
      "           7.6783e-01, -6.4507e-01],\n",
      "         ...,\n",
      "         [ 3.3016e-01, -1.7295e-01, -3.7513e-01,  ...,  4.3112e-01,\n",
      "           2.6788e-01, -1.1980e+00],\n",
      "         [-4.9819e-01,  3.0469e-01,  8.3912e-02,  ...,  1.5467e-01,\n",
      "          -2.3119e-01, -3.8684e-01],\n",
      "         [-9.4715e-01,  4.7605e-02, -1.4507e-01,  ...,  5.1454e-01,\n",
      "           1.7480e-01, -9.1079e-01]]], grad_fn=<AddBackward0>) \n",
      "********Done*******\n",
      "\n",
      "\n",
      "argmax:  [6183, 5638, 2683, 10528, 8795, 12535, 2852, 6645, 3552, 13717, 14466, 7294, 2134, 9056, 370, 13158, 6042, 9827, 5012, 13791, 9487, 4622, 13834, 996, 9487, 13158, 1896, 8448, 9160, 12795, 9292, 6530, 5244, 2575, 6309, 1433, 6183, 5569, 6183, 6309, 6309, 6309, 9718, 2575, 3069, 9718]\n",
      "['enforce', 'havana', 'perfume', 'stranger', 'wanlund', 'hypothermia', 'baker', 'undergo', 'rein', 'relieve', 'unbranded', 'paste', 'putting', 'breakout', 'flight', 'munro', 'bmw', 'lagos', 'banking', 'revoke', 'fennell', 'announcing', 'rosen', 'deputy', 'fennell', 'munro', 'visited', 'philly', 'clearest', 'ko', 'degraded', 'rwanda', 'nipple', 'lafave', 'instantly', 'track', 'enforce', 'eddie', 'enforce', 'instantly', 'instantly', 'instantly', 'incomplete', 'lafave', 'santa', 'incomplete']\n",
      "view adjusted: \n",
      " tensor([[-0.1083,  0.2194, -1.5646,  ...,  0.2577,  0.4523, -0.5548],\n",
      "        [-0.3607,  0.5108, -0.8909,  ...,  0.3491,  0.9462,  0.1504],\n",
      "        [-0.3280, -0.6347, -1.2484,  ...,  0.2319,  0.5812, -0.4938],\n",
      "        ...,\n",
      "        [ 0.3302, -0.1730, -0.3751,  ...,  0.4311,  0.2679, -1.1980],\n",
      "        [-0.4982,  0.3047,  0.0839,  ...,  0.1547, -0.2312, -0.3868],\n",
      "        [-0.9472,  0.0476, -0.1451,  ...,  0.5145,  0.1748, -0.9108]],\n",
      "       grad_fn=<ViewBackward>) \n",
      "*******Done********\n",
      "view-adjusted-shape:  torch.Size([5888, 14744])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (5888) to match target batch_size (5760).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-ab2a958e76b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"view-adjusted-shape: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    914\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m--> 916\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m    917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2019\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2020\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2021\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2023\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1834\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1835\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[1;32m-> 1836\u001b[1;33m                          .format(input.size(0), target.size(0)))\n\u001b[0m\u001b[0;32m   1837\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1838\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (5888) to match target batch_size (5760)."
     ]
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "for i,batch in enumerate(train_iter):\n",
    "    if i == 1:\n",
    "        break\n",
    "    src = batch.text\n",
    "    trg = batch.summ\n",
    "    print(src.shape)\n",
    "    print(trg.shape)\n",
    "    trg_inp, trg_out = trg[:-1, :], trg[1:, :]\n",
    "    \n",
    "    \n",
    "    print(\"text: \",[src_list[i] for i in src.squeeze(1).transpose(0,1)[0].tolist()])\n",
    "    \n",
    "    print(\"\\n\\nsumm: \",[src_list[i] for i in trg.squeeze(1).transpose(0,1)[0].tolist()])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print('trg_inp: ',trg_inp.shape)\n",
    "    print('trg_out: ',trg_out.shape)\n",
    "    \n",
    "    memory = model.transformer.encoder(model.pos_enc(model.embed_src(src) * math.sqrt(model.d_model)))\n",
    "        \n",
    "    out = model.fc(model.transformer.decoder(model.pos_enc(model.embed_tgt(trg) * math.sqrt(model.d_model)), memory))\n",
    "\n",
    "#     out = model(src.to(device),trg_inp.to(device))\n",
    "    \n",
    "#     print(f'out: {out.shape}')\n",
    "#     print(rearrange(out,'t b e -> (b t) e ').shape)\n",
    "    \n",
    "#     print(rearrange(out,'t b e -> (b t) e ').shape)\n",
    "    \n",
    "#     print(rearrange(trg_out, 'o b -> (b o)').shape)\n",
    "    \n",
    "    out_  = rearrange(out,'t b e -> b t e')\n",
    "    \n",
    "    print(\"out:\\n\",out,\"\\n********Done*******\")\n",
    "    \n",
    "    print(\"\\n\\nargmax: \",out_.argmax(2)[0].tolist())\n",
    "    \n",
    "    l = out_.argmax(2)[0].tolist()\n",
    "    \n",
    "    print([src_list[i] for i in l])\n",
    "    \n",
    "    output_dim = out.shape[-1]\n",
    "    \n",
    "    print(\"view adjusted: \\n\",out.view(-1,output_dim),\"\\n*******Done********\")\n",
    "    \n",
    "    print(\"view-adjusted-shape: \",out.view(-1,output_dim).shape)\n",
    "    \n",
    "    loss = criterion(out.view(-1, output_dim), trg_out.view(-1))\n",
    "    \n",
    "    print(loss.item())\n",
    "    \n",
    "    \n",
    "    \n",
    "#     del src,trg,out\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "# del batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(44,127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training......\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab653df10d7482c9f1ce5ee538d1359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  ['<sos>', 'cnn', 'national', 'football', 'league', 'ha', 'indefinitely', 'suspended', 'atlanta', 'falcon', 'quarterback', 'michael', 'vick', 'without', 'pay', 'official', 'league', 'said', 'friday', 'nfl', 'star', 'michael', 'vick', 'set', 'appear', 'court', 'monday', 'judge', 'have', 'final', 'say', 'plea', 'deal', 'earlier', 'vick', 'admitted', 'participating', 'dogfighting', 'ring', 'part', 'plea', 'agreement', 'federal', 'prosecutor', 'virginia', 'your', 'admitted', 'conduct', 'not', 'only', 'illegal', 'but', 'also', 'cruel', 'and', 'reprehensible', 'your', 'team', 'nfl', 'and', 'nfl', 'fan', 'have', 'all', 'been', 'hurt', 'your', 'action', 'nfl', 'commissioner', 'roger', 'goodell', 'said', 'letter', 'vick', 'goodell', 'said', 'he', 'would', 'review', 'status', 'suspension', 'after', 'legal', 'proceeding', 'over', 'paper', 'filed', 'friday', 'federal', 'court', 'virginia', 'vick', 'also', 'admitted', 'he', 'and', 'two', 'coconspirator', 'killed', 'dog', 'did', 'not', 'fight', 'well', 'falcon', 'owner', 'arthur', 'blank', 'said', 'vicks', 'admission', 'describe', 'action', 'and', 'unacceptable', 'suspension', 'make', 'strong', 'statement', 'conduct', 'which', '<unk>', 'good', 'reputation', 'nfl', 'not', '<unk>', 'he', 'said', 'statement', 'watch', 'led', 'vicks', 'suspension', 'goodell', 'said', 'falcon', 'could', 'assert', 'any', 'claim', 'remedy', 'recover', 'million', 'vicks', 'signing', 'bonus', 'year', 'million', 'contract', 'he', 'signed', 'according', 'associated', 'press', 'vick', 'said', 'he', 'would', 'plead', 'guilty', 'one', 'count', 'conspiracy', 'travel', 'interstate', 'commerce', 'aid', 'unlawful', 'activity', 'and', 'sponsor', 'dog', 'animal', 'fighting', 'venture', 'plea', 'agreement', 'filed', 'u', 'district', 'court', 'richmond', 'virginia', 'charge', 'punishable', 'up', 'five', 'year', 'prison', 'fine', 'full', 'restitution', 'special', 'assessment', 'and', 'year', 'supervised', 'release', 'plea', 'deal', 'said', 'federal', 'prosecutor', 'agreed', 'ask', 'low', 'end', 'sentencing', 'guideline', 'defendant', 'plead', 'guilty', 'because', 'defendant', 'fact', 'guilty', 'charged', 'offense', 'plea', 'agreement', 'said', 'additional', 'summary', 'fact', 'signed', 'vick', 'and', 'filed', 'agreement', 'vick', 'admitted', 'buying', 'pit', 'bull', 'and', 'property', 'used', 'training', 'and', 'fighting', 'dog', 'but', 'statement', 'said', 'he', 'did', 'not', 'bet', 'fight', 'receive', 'any', 'money', 'won', 'most', 'bad', 'newz', 'kennel', 'operation', 'and', 'gambling', '<unk>', 'were', 'provided', 'vick', 'official', 'summary', 'fact', 'said', 'gambling', 'win', 'were', 'generally', 'split', 'among', 'coconspirator', 'tony', 'taylor', 'quanis', 'phillips', 'and', 'sometimes', 'purnell', 'peace', 'continued', 'vick', 'did', 'not', 'gamble', 'placing', 'side', 'bet', 'any', 'fight', 'vick', 'did', 'not', 'receive', 'any', 'proceeds', 'purse', 'were', 'won', 'bad', 'newz', 'kennel', 'vick', 'also', 'agreed', 'collective', 'effort', 'him', 'and', 'two', 'others', 'caused', 'death', 'least', 'six', 'dog', 'around', 'april', 'vick', 'peace', 'and', 'phillips', 'tested', 'some', 'dog', 'fighting', 'session', 'vicks', 'property', 'virginia', 'statement', 'said', 'peace', 'phillips', 'and', 'vick', 'agreed', 'killing', 'approximately', 'dog', 'did', 'not', 'perform', 'well', 'testing', 'session', '<unk>', 'road', 'and', 'all', 'those', 'dog', 'were', 'killed', 'various', 'method', 'including', 'hanging', 'and', 'drowning', 'vick', 'agrees', 'and', '<unk>', 'these', 'dog', 'all', 'died', 'result', 'collective', 'effort', 'peace', 'phillips', 'and', 'vick', 'summary', 'said', 'peace', 'virginia', 'beach', 'virginia', 'phillips', 'atlanta', 'georgia', 'and', 'taylor', 'hampton', 'virginia', 'already', 'have', 'accepted', 'agreement', 'plead', 'guilty', 'exchange', 'reduced', 'sentence', 'vick', 'scheduled', 'appear', 'monday', 'court', 'he', 'expected', 'plead', 'guilty', 'before', 'judge', 'see', 'timeline', 'case', 'against', 'vick', 'judge', 'case', 'have', 'final', 'say', 'over', 'plea', 'agreement', 'federal', 'case', 'against', 'vick', 'focused', 'interstate', 'conspiracy', 'but', 'vicks', 'admission', 'he', 'involved', 'killing', 'dog', 'could', 'lead', 'local', 'charge', 'according', 'cnn', 'legal', 'analyst', 'jeffrey', 'toobin', 'sometimes', 'happens', 'not', 'often', 'state', 'follow', 'federal', 'prosecution', 'charging', 'it', 'own', 'crime', 'exactly', 'same', 'behavior', 'toobin', 'said', 'friday', 'risk', 'vick', 'if', 'he', 'make', 'admission', 'his', 'federal', 'guilty', 'plea', 'state', 'virginia', 'could', 'say', 'hey', 'look', 'you', 'admitted', 'violating', 'virginia', 'state', 'law', 'well', 'were', 'going', 'introduce', 'against', 'you', 'and', 'charge', 'you', 'our', 'court', 'plea', 'deal', 'vick', 'agreed', 'cooperate', 'investigator', 'and', 'provide', 'all', 'information', 'he', 'may', 'have', 'any', 'criminal', 'activity', 'and', 'testify', 'if', 'necessary', 'vick', 'also', 'agreed', 'turn', 'over', 'any', 'document', 'he', 'ha', 'and', 'submit', 'polygraph', 'test', 'vick', 'agreed', 'make', 'restitution', 'full', 'amount', 'cost', 'associated', 'dog', 'being', 'held', 'government', 'such', 'cost', 'may', 'include', 'but', 'not', 'limited', 'all', 'cost', 'associated', 'care', 'dog', 'involved', 'case', 'including', 'if', 'necessary', 'longterm', 'care', 'andor', 'humane', '<unk>', 'some', 'all', 'those', 'animal', 'prosecutor', 'support', 'animal', 'right', 'activist', 'have', 'asked', 'permission', '<unk>', 'dog', 'but', 'dog', 'could', 'serve', 'important', 'evidence', 'case', 'against', 'vick', 'and', 'his', 'admitted', 'coconspirator', 'judge', 'henry', 'hudson', 'issued', 'order', 'thursday', 'telling', 'u', 'marshal', 'service', 'arrest', 'and', 'seize', '<eos>']\n",
      "\n",
      "\n",
      "summ:  ['<sos>', 'new', 'nfl', 'chief', 'atlanta', 'falcon', 'owner', 'critical', 'michael', 'vicks', 'conduct', 'nfl', '<unk>', 'falcon', 'quarterback', 'indefinitely', 'without', 'pay', 'vick', 'admits', 'funding', 'dogfighting', 'operation', 'but', 'say', 'he', 'did', 'not', 'gamble', 'vick', 'due', 'federal', 'court', 'monday', 'future', 'nfl', 'remains', 'uncertain', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "src_pad_mask:  tensor([[False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True]]) \n",
      " *****DONE****\n",
      "trg_pad_mask:  tensor([[False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        ...,\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True]]) \n",
      " *****DONE****\n",
      "torch.Size([45, 128, 300])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (5760) to match target batch_size (5632).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-143-75e6ad00f49d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_batches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-137-946fcd754479>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, iterator, num_batches, optimizer, criterion, clip)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    914\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m--> 916\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m    917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2019\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2020\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2021\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2023\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1834\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1835\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[1;32m-> 1836\u001b[1;33m                          .format(input.size(0), target.size(0)))\n\u001b[0m\u001b[0;32m   1837\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1838\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (5760) to match target batch_size (5632)."
     ]
    }
   ],
   "source": [
    "\n",
    "# Running too long\n",
    "# need to fix this\n",
    "\n",
    "def epoch_time(start_time: int,\n",
    "               end_time: int):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "parameters = filter(lambda p:p.requires_grad, model.parameters())\n",
    "optimizer = optim.Adam(parameters)\n",
    "num_batches = math.ceil(len(train_data)/BATCH_SIZE)\n",
    "val_batches = math.ceil(len(val_data)/BATCH_SIZE)\n",
    "\n",
    "N_EPOCHS = 1\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_iter, num_batches,optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, val_iter,val_batches, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f}')\n",
    "    \n",
    "test_size = math.ceil(len(test_data)/BATCH_SIZE)\n",
    "test_loss = evaluate(model, test_iter,test_size, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_text(s):\n",
    "    return [trg_list[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =torch.tensor([[1,0,0],[0,1,0],[0,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 =(x!=0).unsqueeze(1).unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2= torch.tril(torch.ones((x.size(0), x.size(0)), device =device)).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf],\n",
       "        [0., 0., -inf],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ True, False, False]]],\n",
       "\n",
       "\n",
       "        [[[False,  True, False]]],\n",
       "\n",
       "\n",
       "        [[[False, False,  True]]]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 3, 3])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(m1 & m2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single file given as source, rather than a list of files. Wrapping in list.\n",
      "Files read into LineSentenceGenerator: data\\train.source\n",
      "Reading file %s data\\train.source\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'editor note our behind scene series cnn correspondent share their experience covering news and analyze story behind event here soledad obrien take user inside jail many inmate mentally ill inmate housed forgotten floor many mentally ill inmate housed miami before trial miami florida cnn ninth floor miamidade pretrial detention facility dubbed forgotten floor here inmate most severe mental illness incarcerated until theyre ready appear court most often they face drug charge charge assaulting officer charge judge steven leifman say usually avoidable felony he say arrest often result confrontation police mentally ill people often wont do theyre told police arrive scene confrontation seems exacerbate their illness and they become more paranoid delusional and le likely follow direction according leifman so they end up ninth floor severely mentally disturbed but not getting any real help because theyre jail we toured jail leifman he well known miami advocate justice and mentally ill even though we were not exactly welcomed open arm guard we were given permission shoot videotape and tour floor go inside forgotten floor first it hard determine people prisoner wearing sleeveless robe imagine cutting hole arm and foot heavy wool sleeping bag thats kind they look like theyre designed keep mentally ill patient injuring themselves thats also why they have no shoe lace mattress leifman say onethird all people miamidade county jail mentally ill so he say sheer volume overwhelming system and result we see ninth floor course jail so it not supposed warm and comforting but light glare cell tiny and it loud we see two sometimes three men sometimes robe sometimes naked lying sitting their cell am son president you need get me out here one man shout me he absolutely serious convinced help way if only he could reach white house leifman tell me these often circulate through system occasionally stabilizing mental hospital only return jail face their charge it brutally unjust his mind and he ha become strong advocate changing thing miami over meal later we talk thing got way mental patient leifman say year ago people were considered lunatic and they were locked up jail even if they had no charge against them they were just considered unfit society over year he say there some public outcry and mentally ill were moved out jail and into hospital but leifman say many these mental hospital were so horrible they were shut down did patient go nowhere street they became many case homeless he say they never got treatment leifman say there were more than half million people state mental hospital and today number ha been reduced percent and people mental hospital judge say he working change starting many inmate would otherwise have been brought forgotten floor instead sent new mental health facility first step journey toward longterm treatment not just punishment leifman say it not complete answer but it start leifman say best part it winwin solution patient win family relieved and state save money simply not cycling these prisoner through again and again and leifman justice served email friend'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = ''\n",
    "\n",
    "for i,line in enumerate(LineSentenceGenerator(train_file_X,PreProcess)):\n",
    "    if i == 1:\n",
    "        break\n",
    "    s = line\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2, 11957,     3],\n",
       "        [    2,  5099,     3],\n",
       "        [    2,     0,     3],\n",
       "        ...,\n",
       "        [    2, 11957,     3],\n",
       "        [    2, 13191,     3],\n",
       "        [    2,  5099,     3]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_sequence(s):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3115, 1, 3])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.transpose(0,1).unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3115])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
