{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerDecoder,TransformerDecoderLayer\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.nn import Transformer\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mapka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from functools import lru_cache\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.utils as utils\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "cached_lemmatize = lru_cache(maxsize=50000)(WordNetLemmatizer().lemmatize)\n",
    "from gensim.utils import simple_preprocess, to_unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_dim,emb_dim,enc_hid_dim,dec_hid_dim,dropout=0.5):\n",
    "        \n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim,emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        \n",
    "        self.fc = nn.Linear( enc_hid_dim * 2, dec_hid_dim )\n",
    "        \n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(X))\n",
    "        \n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        \n",
    "        hidden = F.tanh( self.fc ( torch.cat( (hidden[-2,:,:], hidden[-1, : , : ] ), dim = 1 ) ) )\n",
    "        \n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"data\"\n",
    "train_file_X = os.path.join(base_dir,\"train.source\")\n",
    "train_file_y = os.path.join(base_dir,\"train.target\")\n",
    "test_file_X = os.path.join(base_dir,\"test.source\")\n",
    "test_file_y = os.path.join(base_dir,\"test.target\")\n",
    "val_file_X = os.path.join(base_dir,\"val.source\")\n",
    "val_file_y = os.path.join(base_dir,\"val.target\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "STOP_WORDS = [\"i\", \"a\", \"about\", \"an\", \"are\", \"as\", \"at\", \"be\", \"by\", \n",
    "                \"for\", \"from\", \"how\", \"in\", \"is\", \"it\", \"of\", \"on\", \"or\", \"that\", \"the\", \n",
    "                \"this\", \"to\", \"was\", \"what\", \"when\", \"where\", \"who\", \"will\", \"with\"]\n",
    "\n",
    "def ExpandContractions(contraction):\n",
    "\n",
    "    contraction = re.sub(r\"won\\'t\", \"will not\", contraction)\n",
    "    contraction = re.sub(r\"can\\'t\", \"can not\", contraction)\n",
    "\n",
    "    contraction = re.sub(r\"n\\'t\", \" not\", contraction)\n",
    "    contraction = re.sub(r\"\\'re\", \" are\", contraction)\n",
    "    contraction = re.sub(r\"\\'s\", \" is\", contraction)\n",
    "    contraction = re.sub(r\"\\'d\", \" would\", contraction)\n",
    "    contraction = re.sub(r\"\\'ll\", \" will\", contraction)\n",
    "    contraction = re.sub(r\"\\'t\", \" not\", contraction)\n",
    "    contraction = re.sub(r\"\\'ve\", \" have\", contraction)\n",
    "    contraction = re.sub(r\"\\'m\", \" am\", contraction)\n",
    "\n",
    "    return contraction\n",
    "\n",
    "def PreProcess(line):\n",
    "    \n",
    "    line = line.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    line = ExpandContractions(line)\n",
    "    line = simple_preprocess(to_unicode(line))\n",
    "    line = [cached_lemmatize(word) for word in line if word not in STOP_WORDS]\n",
    "\n",
    "    line = \" \".join(line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineSentenceGenerator(object):\n",
    "\n",
    "    def __init__(self, source, preprocess=None, max_sentence_length=4000, limit=None, preprocess_flag=True):\n",
    "        self.source = source\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.limit = limit\n",
    "        self.input_files = []\n",
    "\n",
    "        if preprocess != None and callable(preprocess) and preprocess_flag:\n",
    "            self.preprocess = preprocess\n",
    "        else:\n",
    "            self.preprocess = lambda line: line.rstrip(\"\\r\\n\")\n",
    "\n",
    "        if isinstance(self.source, list):\n",
    "            print('List of files given as source. Verifying entries and using.')\n",
    "            self.input_files = [filename for filename in self.source if os.path.isfile(filename)]\n",
    "            self.input_files.sort()  # makes sure it happens in filename order\n",
    "\n",
    "        elif os.path.isfile(self.source):\n",
    "            print('Single file given as source, rather than a list of files. Wrapping in list.')\n",
    "            self.input_files = [self.source]  # force code compatibility with list of files\n",
    "\n",
    "        elif os.path.isdir(self.source):\n",
    "            self.source = os.path.join(self.source, '')  # ensures os-specific slash at end of path\n",
    "            print('Directory of files given as source. Reading directory %s', self.source)\n",
    "            self.input_files = os.listdir(self.source)\n",
    "            self.input_files = [self.source + filename for filename in self.input_files]  # make full paths\n",
    "            self.input_files.sort()  # makes sure it happens in filename order\n",
    "        else:  # not a file or a directory, then we can't do anything with it\n",
    "            raise ValueError('Input is neither a file nor a path nor a list')\n",
    "        print('Files read into LineSentenceGenerator: %s' % ('\\n'.join(self.input_files)))\n",
    "\n",
    "        self.token_count = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        for file_name in self.input_files:\n",
    "            print('Reading file %s', file_name)\n",
    "            with open(file_name, 'rb') as fin:\n",
    "                for line in itertools.islice(fin, self.limit):\n",
    "                    line = self.preprocess(utils.to_unicode(line))\n",
    "                    self.token_count += len(line)\n",
    "                    i = 0\n",
    "                    while i < len(line):\n",
    "                        yield line[i:i + self.max_sentence_length]\n",
    "                        i += self.max_sentence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.token_count > 0:\n",
    "            return self.token_count\n",
    "        else:\n",
    "            return len(self.input_files)\n",
    "\n",
    "    def __bool__(self):\n",
    "        return self.has_data()\n",
    "\n",
    "    def is_empty(self):\n",
    "        return len(self.input_files) == 0\n",
    "\n",
    "    def has_data(self):\n",
    "        return not self.is_empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of files given as source. Verifying entries and using.\n",
      "Files read into LineSentenceGenerator: data\\train.source\n",
      "data\\train.target\n",
      "Reading file %s data\\train.source\n",
      "3115\n",
      "editor note our behind scene series cnn correspondent share their experience covering news and analyze story behind event here soledad obrien take user inside jail many inmate mentally ill inmate housed forgotten floor many mentally ill inmate housed miami before trial miami florida cnn ninth floor miamidade pretrial detention facility dubbed forgotten floor here inmate most severe mental illness incarcerated until theyre ready appear court most often they face drug charge charge assaulting officer charge judge steven leifman say usually avoidable felony he say arrest often result confrontation police mentally ill people often wont do theyre told police arrive scene confrontation seems exacerbate their illness and they become more paranoid delusional and le likely follow direction according leifman so they end up ninth floor severely mentally disturbed but not getting any real help because theyre jail we toured jail leifman he well known miami advocate justice and mentally ill even though we were not exactly welcomed open arm guard we were given permission shoot videotape and tour floor go inside forgotten floor first it hard determine people prisoner wearing sleeveless robe imagine cutting hole arm and foot heavy wool sleeping bag thats kind they look like theyre designed keep mentally ill patient injuring themselves thats also why they have no shoe lace mattress leifman say onethird all people miamidade county jail mentally ill so he say sheer volume overwhelming system and result we see ninth floor course jail so it not supposed warm and comforting but light glare cell tiny and it loud we see two sometimes three men sometimes robe sometimes naked lying sitting their cell am son president you need get me out here one man shout me he absolutely serious convinced help way if only he could reach white house leifman tell me these often circulate through system occasionally stabilizing mental hospital only return jail face their charge it brutally unjust his mind and he ha become strong advocate changing thing miami over meal later we talk thing got way mental patient leifman say year ago people were considered lunatic and they were locked up jail even if they had no charge against them they were just considered unfit society over year he say there some public outcry and mentally ill were moved out jail and into hospital but leifman say many these mental hospital were so horrible they were shut down did patient go nowhere street they became many case homeless he say they never got treatment leifman say there were more than half million people state mental hospital and today number ha been reduced percent and people mental hospital judge say he working change starting many inmate would otherwise have been brought forgotten floor instead sent new mental health facility first step journey toward longterm treatment not just punishment leifman say it not complete answer but it start leifman say best part it winwin solution patient win family relieved and state save money simply not cycling these prisoner through again and again and leifman justice served email friend\n",
      "Single file given as source, rather than a list of files. Wrapping in list.\n",
      "Files read into LineSentenceGenerator: data\\train.target\n",
      "Reading file %s data\\train.target\n",
      "\n",
      "\n",
      " mentally ill inmate miami housed forgotten floor judge steven leifman say most there result avoidable felony while cnn tour facility patient shout am son president leifman say system unjust and he fighting change\n"
     ]
    }
   ],
   "source": [
    "for i,line in enumerate(LineSentenceGenerator([train_file_X,train_file_y],PreProcess)):\n",
    "    if i == 1:\n",
    "        break\n",
    "    print(len(line))\n",
    "    print(line)\n",
    "\n",
    "for i,line in enumerate(LineSentenceGenerator(train_file_y,PreProcess)):\n",
    "    if i == 1:\n",
    "        break\n",
    "    print(\"\\n\\n\",line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Dataset,Example\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "SRC = Field(tokenize = \"spacy\",\n",
    "            lower = False)\n",
    "\n",
    "TRG = Field(tokenize = \"spacy\",\n",
    "            is_target = True,\n",
    "            lower = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(X,y,limit=1000):\n",
    "    examples = []\n",
    "    fields = {'text-tokens': ('text', SRC),\n",
    "              'summ-tokens': ('summ', TRG)}\n",
    "    for i,(x,y) in enumerate(zip(LineSentenceGenerator(X,PreProcess),LineSentenceGenerator(y,PreProcess))):\n",
    "        if i > limit:\n",
    "            break\n",
    "        text_field = x\n",
    "        summ_field = y\n",
    "       \n",
    "        e = Example.fromdict({\"text-tokens\": text_field, \"summ-tokens\": summ_field},\n",
    "                             fields=fields)\n",
    "        examples.append(e)\n",
    "    print(\"examples: \\n\", examples[0])\n",
    "    return Dataset(examples, fields=[('text', SRC), ('summ', TRG)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single file given as source, rather than a list of files. Wrapping in list.\n",
      "Files read into LineSentenceGenerator: data\\train.source\n",
      "Single file given as source, rather than a list of files. Wrapping in list.\n",
      "Files read into LineSentenceGenerator: data\\train.target\n",
      "Reading file %s data\\train.source\n",
      "Reading file %s data\\train.target\n",
      "examples: \n",
      " <torchtext.data.example.Example object at 0x0000017190E91448>\n",
      "Single file given as source, rather than a list of files. Wrapping in list.\n",
      "Files read into LineSentenceGenerator: data\\test.source\n",
      "Single file given as source, rather than a list of files. Wrapping in list.\n",
      "Files read into LineSentenceGenerator: data\\test.target\n",
      "Reading file %s data\\test.source\n",
      "Reading file %s data\\test.target\n",
      "examples: \n",
      " <torchtext.data.example.Example object at 0x00000171A7CB2D08>\n",
      "Single file given as source, rather than a list of files. Wrapping in list.\n",
      "Files read into LineSentenceGenerator: data\\val.source\n",
      "Single file given as source, rather than a list of files. Wrapping in list.\n",
      "Files read into LineSentenceGenerator: data\\val.target\n",
      "Reading file %s data\\val.source\n",
      "Reading file %s data\\val.target\n",
      "examples: \n",
      " <torchtext.data.example.Example object at 0x00000171A7CB2C88>\n"
     ]
    }
   ],
   "source": [
    "train_data = read_data(train_file_X,train_file_y,1000)\n",
    "test_data = read_data(test_file_X,test_file_y,200)\n",
    "val_data = read_data(val_file_X,val_file_y,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  ['editor', 'note', 'our', 'behind', 'scene', 'series', 'cnn', 'correspondent', 'share', 'their', 'experience', 'covering', 'news', 'and', 'analyze', 'story', 'behind', 'event', 'here', 'soledad', 'obrien', 'take', 'user', 'inside', 'jail', 'many', 'inmate', 'mentally', 'ill', 'inmate', 'housed', 'forgotten', 'floor', 'many', 'mentally', 'ill', 'inmate', 'housed', 'miami', 'before', 'trial', 'miami', 'florida', 'cnn', 'ninth', 'floor', 'miamidade', 'pretrial', 'detention', 'facility', 'dubbed', 'forgotten', 'floor', 'here', 'inmate', 'most', 'severe', 'mental', 'illness', 'incarcerated', 'until', 'they', 're', 'ready', 'appear', 'court', 'most', 'often', 'they', 'face', 'drug', 'charge', 'charge', 'assaulting', 'officer', 'charge', 'judge', 'steven', 'leifman', 'say', 'usually', 'avoidable', 'felony', 'he', 'say', 'arrest', 'often', 'result', 'confrontation', 'police', 'mentally', 'ill', 'people', 'often', 'wo', 'nt', 'do', 'they', 're', 'told', 'police', 'arrive', 'scene', 'confrontation', 'seems', 'exacerbate', 'their', 'illness', 'and', 'they', 'become', 'more', 'paranoid', 'delusional', 'and', 'le', 'likely', 'follow', 'direction', 'according', 'leifman', 'so', 'they', 'end', 'up', 'ninth', 'floor', 'severely', 'mentally', 'disturbed', 'but', 'not', 'getting', 'any', 'real', 'help', 'because', 'they', 're', 'jail', 'we', 'toured', 'jail', 'leifman', 'he', 'well', 'known', 'miami', 'advocate', 'justice', 'and', 'mentally', 'ill', 'even', 'though', 'we', 'were', 'not', 'exactly', 'welcomed', 'open', 'arm', 'guard', 'we', 'were', 'given', 'permission', 'shoot', 'videotape', 'and', 'tour', 'floor', 'go', 'inside', 'forgotten', 'floor', 'first', 'it', 'hard', 'determine', 'people', 'prisoner', 'wearing', 'sleeveless', 'robe', 'imagine', 'cutting', 'hole', 'arm', 'and', 'foot', 'heavy', 'wool', 'sleeping', 'bag', 'that', 's', 'kind', 'they', 'look', 'like', 'they', 're', 'designed', 'keep', 'mentally', 'ill', 'patient', 'injuring', 'themselves', 'that', 's', 'also', 'why', 'they', 'have', 'no', 'shoe', 'lace', 'mattress', 'leifman', 'say', 'onethird', 'all', 'people', 'miamidade', 'county', 'jail', 'mentally', 'ill', 'so', 'he', 'say', 'sheer', 'volume', 'overwhelming', 'system', 'and', 'result', 'we', 'see', 'ninth', 'floor', 'course', 'jail', 'so', 'it', 'not', 'supposed', 'warm', 'and', 'comforting', 'but', 'light', 'glare', 'cell', 'tiny', 'and', 'it', 'loud', 'we', 'see', 'two', 'sometimes', 'three', 'men', 'sometimes', 'robe', 'sometimes', 'naked', 'lying', 'sitting', 'their', 'cell', 'am', 'son', 'president', 'you', 'need', 'get', 'me', 'out', 'here', 'one', 'man', 'shout', 'me', 'he', 'absolutely', 'serious', 'convinced', 'help', 'way', 'if', 'only', 'he', 'could', 'reach', 'white', 'house', 'leifman', 'tell', 'me', 'these', 'often', 'circulate', 'through', 'system', 'occasionally', 'stabilizing', 'mental', 'hospital', 'only', 'return', 'jail', 'face', 'their', 'charge', 'it', 'brutally', 'unjust', 'his', 'mind', 'and', 'he', 'ha', 'become', 'strong', 'advocate', 'changing', 'thing', 'miami', 'over', 'meal', 'later', 'we', 'talk', 'thing', 'got', 'way', 'mental', 'patient', 'leifman', 'say', 'year', 'ago', 'people', 'were', 'considered', 'lunatic', 'and', 'they', 'were', 'locked', 'up', 'jail', 'even', 'if', 'they', 'had', 'no', 'charge', 'against', 'them', 'they', 'were', 'just', 'considered', 'unfit', 'society', 'over', 'year', 'he', 'say', 'there', 'some', 'public', 'outcry', 'and', 'mentally', 'ill', 'were', 'moved', 'out', 'jail', 'and', 'into', 'hospital', 'but', 'leifman', 'say', 'many', 'these', 'mental', 'hospital', 'were', 'so', 'horrible', 'they', 'were', 'shut', 'down', 'did', 'patient', 'go', 'nowhere', 'street', 'they', 'became', 'many', 'case', 'homeless', 'he', 'say', 'they', 'never', 'got', 'treatment', 'leifman', 'say', 'there', 'were', 'more', 'than', 'half', 'million', 'people', 'state', 'mental', 'hospital', 'and', 'today', 'number', 'ha', 'been', 'reduced', 'percent', 'and', 'people', 'mental', 'hospital', 'judge', 'say', 'he', 'working', 'change', 'starting', 'many', 'inmate', 'would', 'otherwise', 'have', 'been', 'brought', 'forgotten', 'floor', 'instead', 'sent', 'new', 'mental', 'health', 'facility', 'first', 'step', 'journey', 'toward', 'longterm', 'treatment', 'not', 'just', 'punishment', 'leifman', 'say', 'it', 'not', 'complete', 'answer', 'but', 'it', 'start', 'leifman', 'say', 'best', 'part', 'it', 'winwin', 'solution', 'patient', 'win', 'family', 'relieved', 'and', 'state', 'save', 'money', 'simply', 'not', 'cycling', 'these', 'prisoner', 'through', 'again', 'and', 'again', 'and', 'leifman', 'justice', 'served', 'email', 'friend']\n",
      "\n",
      "\n",
      "summary:  ['mentally', 'ill', 'inmate', 'miami', 'housed', 'forgotten', 'floor', 'judge', 'steven', 'leifman', 'say', 'most', 'there', 'result', 'avoidable', 'felony', 'while', 'cnn', 'tour', 'facility', 'patient', 'shout', 'am', 'son', 'president', 'leifman', 'say', 'system', 'unjust', 'and', 'he', 'fighting', 'change']\n"
     ]
    }
   ],
   "source": [
    "print(\"text: \",train_data[0].text)\n",
    "print(\"\\n\\nsummary: \",train_data[0].summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': <torchtext.data.field.Field at 0x1718dee9f88>,\n",
       " 'summ': <torchtext.data.field.Field at 0x1718df43e48>}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  ['cnna', 'frenchlanguage', 'global', 'television', 'network', 'regained', 'control', 'one', 'it', 'channel', 'thursday', 'after', 'cyberattack', 'day', 'earlier', 'crippled', 'it', 'broadcast', 'and', 'social', 'medium', 'account', 'television', 'network', 'tv', 'monde', 'gradually', 'regaining', 'control', 'it', 'channel', 'and', 'social', 'medium', 'outlet', 'after', 'suffering', 'network', 'director', 'called', 'extremely', 'powerful', 'cyberattack', 'addition', 'it', 'channel', 'tv', 'monde', 'lost', 'control', 'it', 'social', 'medium', 'outlet', 'and', 'it', 'website', 'director', 'yves', 'bigot', 'said', 'video', 'message', 'posted', 'later', 'facebook', 'mobile', 'site', 'which', 'still', 'active', 'network', 'said', 'hacked', 'islamist', 'group', 'isi', 'logo', 'and', 'marking', 'appeared', 'tv', 'monde', 'social', 'medium', 'account', 'but', 'there', 'no', 'immediate', 'claim', 'responsibility', 'isi', 'any', 'other', 'group', 'day', 'broke', 'thursday', 'europe', 'network', 'had', 'regained', 'use', 'one', 'it', 'channel', 'and', 'it', 'facebook', 'page', 'paul', 'germain', 'chain', 'editor', 'chief', 'told', 'bfmtv', 'cnn', 'affiliate', 'france', 'however', 'late', 'morning', 'number', 'page', 'network', 'website', 'had', 'message', 'saying', 'they', 'were', 'under', 'maintenance', 'outage', 'began', 'around', 'pm', 'paris', 'time', 'pm', 'et', 'wednesday', 'tv', 'monde', 'offer', 'roundtheclock', 'entertainment', 'and', 'news', 'programming', 'reach', 'million', 'home', 'worldwide', 'according', 'ministry', 'culture', 'and', 'communication', 'function', 'under', 'partnership', 'among', 'government', 'france', 'canada', 'and', 'switzerland', 'well', 'federation', 'other', 'network', 'provide', 'content', 'tv', 'monde', 'include', 'cnn', 'affiliate', 'france', 'and', 'france', 'france', 'and', 'radio', 'france', 'international']\n",
      "summ:  ['don', 'mcleans', 'american', 'pie', 'lyric', 'auctioned', 'million', 'song', 'dense', 'symbolism', 'mclean', 'say', 'lyric', 'note', 'reveal', 'meaning', 'pie', 'mcleans', 'biggest', 'hit', 'no']\n"
     ]
    }
   ],
   "source": [
    "print(\"text: \", test_data[100].text)\n",
    "print(\"summ: \",test_data[100].summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data.text, min_freq = 2,max_size=4000)\n",
    "TRG.build_vocab(train_data.summ, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iter = BucketIterator(train_data,BATCH_SIZE, shuffle=True,\n",
    "                                                 sort_key=lambda x: len(x.text), sort_within_batch=True)\n",
    "\n",
    "val_iter = BucketIterator(val_data, BATCH_SIZE, sort_key=lambda x: len(x.text), sort_within_batch=True)\n",
    "test_iter = BucketIterator(test_data,BATCH_SIZE, sort_key=lambda x: len(x.text), sort_within_batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([320, 128])\n",
      "torch.Size([48, 128])\n",
      "torch.Size([398, 128])\n",
      "torch.Size([44, 128])\n",
      "torch.Size([691, 128])\n",
      "torch.Size([43, 128])\n",
      "torch.Size([1225, 105])\n",
      "torch.Size([54, 105])\n",
      "torch.Size([261, 128])\n",
      "torch.Size([44, 128])\n",
      "torch.Size([475, 128])\n",
      "torch.Size([43, 128])\n",
      "torch.Size([196, 128])\n",
      "torch.Size([43, 128])\n",
      "torch.Size([565, 128])\n",
      "torch.Size([44, 128])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_iter:\n",
    "    print(batch.text.size())\n",
    "    print(batch.summ.size())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class TransformerSummarizer(nn.Module):\n",
    "    def __init__(self, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length,vocab_size,d_model=None, pos_dropout =0.1, trans_dropout= 0.1,embeddings=None):\n",
    "        super().__init__()\n",
    "       \n",
    "        if embeddings is None:\n",
    "            self.embed_src = nn.Embedding(vocab_size, d_model)\n",
    "            self.embed_tgt = nn.Embedding(vocab_size, d_model)\n",
    "        else:\n",
    "            d_model = embeddings.size(1)\n",
    "            self.d_model = embeddings.size(1)\n",
    "            self.embed_src = nn.Embedding(*embeddings.shape)\n",
    "            self.embed_src.weight = nn.Parameter(embeddings,requires_grad=False)\n",
    "            \n",
    "            self.embed_tgt = nn.Embedding(*embeddings.shape)\n",
    "            self.embed_tgt.weight = nn.Parameter(embeddings,requires_grad=False)\n",
    "        \n",
    "        \n",
    "        self.pos_enc = PositionalEncoding(d_model, pos_dropout, max_seq_length)\n",
    "\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, trans_dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \n",
    "#         print(\"Before Embed: \",src.shape,tgt.shape,sep=\"\\n\")\n",
    "        \n",
    "        src = self.pos_enc(self.embed_src(src) * math.sqrt(self.d_model))\n",
    "#         print(src.shape)\n",
    "        tgt = self.pos_enc(self.embed_tgt(tgt) * math.sqrt(self.d_model))\n",
    "#         print(tgt.shape)\n",
    "\n",
    "        output = self.transformer(src, tgt)\n",
    "        \n",
    "        return F.softmax(self.fc(output),dim=2)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_mask(self,sz):\n",
    "        mask = (torch.triu(torch.ones(sz,sz)) == 1).transpose(0,1)\n",
    "        mask = mask.float().masked_fill_(mask == 0,float('-inf')).masked_fill_(mask == 1,float(0.0))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4002\n"
     ]
    }
   ],
   "source": [
    "# trg = len(TRG.vocab)\n",
    "# EMB_DIM = 200\n",
    "SEQ_LEN = 4000\n",
    "\n",
    "D_MODEL = 200 #embedding_size\n",
    "DIM_FEEDFORWARD = 200\n",
    "VOCAB_SIZE = len(SRC.vocab)\n",
    "print(VOCAB_SIZE)\n",
    "ATTENTION_HEADS = 6\n",
    "N_LAYERS = 1\n",
    "\n",
    "# vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length, pos_dropout, trans_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import FastText\n",
    "\n",
    "ff = FastText(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = ff.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = vectors = ff.get_vecs_by_tokens(SRC.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerSummarizer( ATTENTION_HEADS,N_LAYERS, N_LAYERS, DIM_FEEDFORWARD, SEQ_LEN,VOCAB_SIZE,embeddings=embeddings).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerSummarizer(\n",
       "  (embed_src): Embedding(4002, 300)\n",
       "  (embed_tgt): Embedding(4002, 300)\n",
       "  (pos_enc): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=300, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=200, out_features=300, bias=True)\n",
       "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=300, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=200, out_features=300, bias=True)\n",
       "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=300, out_features=4002, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "def train(model: nn.Module,\n",
    "          iterator: BucketIterator,\n",
    "          num_batches: int,\n",
    "          optimizer: optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float):\n",
    "    \n",
    "    print(\"Training......\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in tqdm(iterator,total=num_batches):\n",
    "        \n",
    "#         if i == 1:\n",
    "#             break\n",
    "\n",
    "        src = batch.text\n",
    "        trg = batch.summ\n",
    "        \n",
    "#         tgt_inp, tgt_out = tgt[:, :-1], tgt[:, 1:]\n",
    "#         tgt_mask = gen_nopeek_mask(tgt_inp.shape[1]).to('cuda')\n",
    "\n",
    "#         trg_inp = trg[:,:-1] \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src.to(device), trg.to(device))\n",
    "\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    print(\"Training Done.....\")\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module,\n",
    "             iterator: BucketIterator,\n",
    "             num_batches:int,\n",
    "             criterion: nn.Module):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    print(\"Evaluating....\")\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in tqdm(iterator,total=num_batches):\n",
    "            \n",
    "#             if i == 1:\n",
    "#                 break\n",
    "            src = batch.text\n",
    "            trg = batch.summ\n",
    "\n",
    "            output = model(src.to(device), trg.to(device))\n",
    "\n",
    "            \n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        print(\"Evaluating Done........\")\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([352, 128])\n",
      "torch.Size([43, 128])\n",
      "tensor([[[2.4626e-04, 3.3160e-04, 4.3547e-04,  ..., 3.9895e-04,\n",
      "          1.8387e-04, 2.3146e-04],\n",
      "         [3.0161e-04, 2.6841e-04, 1.7282e-04,  ..., 1.8614e-04,\n",
      "          2.0611e-04, 2.2470e-04],\n",
      "         [3.7614e-04, 1.4737e-04, 4.6549e-04,  ..., 1.7094e-04,\n",
      "          2.1025e-04, 1.0706e-04],\n",
      "         ...,\n",
      "         [3.2146e-04, 1.4434e-04, 2.3186e-04,  ..., 2.5634e-04,\n",
      "          5.2067e-05, 1.8164e-04],\n",
      "         [4.7511e-04, 1.8989e-04, 1.6362e-04,  ..., 2.1485e-04,\n",
      "          2.8546e-04, 3.2417e-04],\n",
      "         [5.5858e-04, 1.1187e-04, 3.2009e-04,  ..., 1.6644e-04,\n",
      "          8.1045e-05, 8.8322e-05]],\n",
      "\n",
      "        [[2.2275e-04, 1.4432e-04, 3.3426e-04,  ..., 1.5151e-04,\n",
      "          8.3821e-05, 1.6988e-04],\n",
      "         [2.4563e-04, 2.6211e-04, 4.5522e-04,  ..., 1.3766e-04,\n",
      "          3.6687e-04, 1.6658e-04],\n",
      "         [2.1509e-04, 2.6720e-04, 1.3584e-04,  ..., 1.8691e-04,\n",
      "          1.4094e-04, 1.2939e-04],\n",
      "         ...,\n",
      "         [5.1403e-04, 2.4544e-04, 1.4474e-04,  ..., 2.7156e-04,\n",
      "          1.2415e-04, 1.5239e-04],\n",
      "         [3.4516e-04, 2.3359e-04, 2.3888e-04,  ..., 1.6347e-04,\n",
      "          2.0681e-04, 8.0357e-05],\n",
      "         [4.5263e-04, 3.3856e-04, 3.1317e-04,  ..., 1.1439e-04,\n",
      "          1.5656e-04, 1.0258e-04]],\n",
      "\n",
      "        [[9.4104e-04, 1.0689e-04, 3.2992e-04,  ..., 2.4731e-04,\n",
      "          1.0157e-04, 4.6273e-04],\n",
      "         [1.9531e-04, 1.7851e-04, 3.0613e-04,  ..., 2.8617e-04,\n",
      "          1.6565e-04, 7.6389e-05],\n",
      "         [1.8396e-04, 8.8115e-05, 1.4167e-04,  ..., 2.5855e-04,\n",
      "          2.0513e-04, 4.2714e-04],\n",
      "         ...,\n",
      "         [5.4942e-04, 1.6120e-04, 2.7132e-04,  ..., 1.6620e-04,\n",
      "          9.1607e-05, 2.5200e-04],\n",
      "         [6.2877e-04, 1.2090e-04, 4.1838e-04,  ..., 1.6993e-04,\n",
      "          7.9392e-05, 2.8327e-04],\n",
      "         [2.1423e-04, 2.0703e-04, 1.2439e-04,  ..., 2.5771e-04,\n",
      "          1.3799e-04, 1.5016e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[3.0599e-04, 8.9959e-05, 1.8914e-04,  ..., 3.0170e-04,\n",
      "          1.4933e-04, 4.3847e-04],\n",
      "         [1.6937e-04, 1.1326e-04, 2.2152e-04,  ..., 8.7023e-04,\n",
      "          1.1272e-04, 3.3708e-04],\n",
      "         [3.3240e-04, 1.3510e-04, 1.3992e-04,  ..., 1.4630e-04,\n",
      "          1.4205e-04, 1.5384e-04],\n",
      "         ...,\n",
      "         [3.8788e-04, 1.2925e-04, 1.1604e-04,  ..., 1.9884e-04,\n",
      "          1.4949e-04, 2.2042e-04],\n",
      "         [2.0237e-04, 1.2324e-04, 2.5870e-04,  ..., 2.6309e-04,\n",
      "          1.0742e-04, 2.1951e-04],\n",
      "         [3.7334e-04, 1.7216e-04, 1.1535e-04,  ..., 4.0044e-04,\n",
      "          8.2249e-05, 1.9794e-04]],\n",
      "\n",
      "        [[2.3201e-04, 1.9636e-04, 1.7046e-04,  ..., 2.4131e-04,\n",
      "          2.0907e-04, 2.1644e-04],\n",
      "         [4.5887e-04, 1.5262e-04, 1.4957e-04,  ..., 3.8885e-04,\n",
      "          5.4473e-05, 1.7460e-04],\n",
      "         [3.2990e-04, 1.8646e-04, 1.3454e-04,  ..., 2.8264e-04,\n",
      "          1.2117e-04, 1.5328e-04],\n",
      "         ...,\n",
      "         [6.8914e-04, 9.8118e-05, 1.0781e-04,  ..., 2.0441e-04,\n",
      "          6.2185e-05, 2.5053e-04],\n",
      "         [9.8402e-05, 1.1091e-04, 3.4133e-04,  ..., 3.1559e-04,\n",
      "          1.2338e-04, 2.8851e-04],\n",
      "         [2.7253e-04, 2.0021e-04, 2.0363e-04,  ..., 3.6997e-04,\n",
      "          1.0159e-04, 1.8399e-04]],\n",
      "\n",
      "        [[2.8138e-04, 7.6209e-05, 1.3873e-04,  ..., 2.6892e-04,\n",
      "          9.8160e-05, 1.4344e-04],\n",
      "         [2.4992e-04, 1.9336e-04, 1.5831e-04,  ..., 2.5529e-04,\n",
      "          6.2548e-05, 1.9569e-04],\n",
      "         [4.2535e-04, 1.3776e-04, 2.9660e-04,  ..., 2.0406e-04,\n",
      "          8.5590e-05, 2.4891e-04],\n",
      "         ...,\n",
      "         [4.7235e-04, 1.1438e-04, 7.3110e-05,  ..., 2.0636e-04,\n",
      "          1.5982e-04, 2.9851e-04],\n",
      "         [2.6961e-04, 1.1030e-04, 2.3688e-04,  ..., 2.5228e-04,\n",
      "          8.3828e-05, 1.5789e-04],\n",
      "         [2.3231e-04, 1.2334e-04, 2.7294e-04,  ..., 6.4043e-04,\n",
      "          6.4738e-05, 1.8111e-04]]], grad_fn=<SliceBackward>)\n",
      "torch.Size([43, 128, 4002])\n"
     ]
    }
   ],
   "source": [
    "for i,batch in enumerate(train_iter):\n",
    "    if i == 1:\n",
    "        break\n",
    "    src = batch.text\n",
    "    trg = batch.summ\n",
    "    print(src.shape)\n",
    "    print(trg.shape)\n",
    "#     trg_inp, trg_out = trg[:, :-1], trg[:, 1:]\n",
    "#     print(\"trg-inp-shape: \",trg_inp.shape)\n",
    "# #     print(\"trg-out-shape: \",trg_out.shape)\n",
    "#     print(\"trg-inp: \",trg_inp)\n",
    "#     print(\"trg-out: \",trg_out)\n",
    "    out = model(src.to(device),trg.to(device))\n",
    "    print(out[0:])\n",
    "    print(out.shape)\n",
    "    \n",
    "    \n",
    "#     del src,trg,out\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "# del batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(44,127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training......\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c809bd47fcd448b285e60d30178d45d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Done.....\n",
      "Evaluating....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f193c380cdd4345bb34faa3a73d3952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Done........\n",
      "Epoch: 01 | Time: 1m 46s\n",
      "\tTrain Loss: 7.365 | Train PPL: 1579.357\n",
      "\t Val. Loss: 5.283 |  Val. PPL: 196.864\n",
      "Evaluating....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2914905dbf47c0b5f62eab241f4de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Done........\n",
      "| Test Loss: 5.363 | Test PPL: 213.281 |\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Running too long\n",
    "# need to fix this\n",
    "\n",
    "def epoch_time(start_time: int,\n",
    "               end_time: int):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "parameters = filter(lambda p:p.requires_grad, model.parameters())\n",
    "optimizer = optim.Adam(parameters)\n",
    "num_batches = math.ceil(len(train_data)/BATCH_SIZE)\n",
    "val_batches = math.ceil(len(val_data)/BATCH_SIZE)\n",
    "\n",
    "N_EPOCHS = 1\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_iter, num_batches,optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, val_iter,val_batches, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "test_size = math.ceil(len(test_data)/BATCH_SIZE)\n",
    "test_loss = evaluate(model, test_iter,test_size, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4002, 300])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
