{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from torchtext.data import Dataset,Example\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "from einops import rearrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from data import get_src_trg, read_data\n",
    "from model import TransformerSummarizer\n",
    "from trainers import train, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "SEQ_LEN = 4000\n",
    "\n",
    "TRAIN_SIZE = 1000\n",
    "TEST_SIZE = 200\n",
    "VAL_SIZE = 200\n",
    "\n",
    "D_MODEL = 300 # Embedding dimension\n",
    "DIM_FEEDFORWARD = 300  # Dimensionality of the hidden state\n",
    "\n",
    "ATTENTION_HEADS = 6  # number of attention heads\n",
    "N_LAYERS = 1 # number of encoder/decoder layers\n",
    "\n",
    "N_EPOCHS = 1\n",
    "CLIP = 1\n",
    "\n",
    "device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.join(os.getcwd(), \"data\")\n",
    "# base_dir = os.path.join(os.getcwd(), \"cnn_dm\")\n",
    "train_file_X = os.path.join(base_dir,\"train.source\")\n",
    "train_file_y = os.path.join(base_dir,\"train.target\")\n",
    "test_file_X = os.path.join(base_dir,\"test.source\")\n",
    "test_file_y = os.path.join(base_dir,\"test.target\")\n",
    "val_file_X = os.path.join(base_dir,\"val.source\")\n",
    "val_file_y = os.path.join(base_dir,\"val.target\")\n",
    "\n",
    "out_dir = os.path.join(os.getcwd(), \"results\", \"transformer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC, TRG = get_src_trg(True)\n",
    "VOCAB_SIZE = len(SRC.vocab)  # size of the vocabulary\n",
    "print(\"vocab_size: \", VOCAB_SIZE) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_data(train_file_X, train_file_y, SRC, SRC, PreProcessMinimal, TRAIN_SIZE)\n",
    "test_data = read_data(test_file_X, test_file_y, SRC, SRC, PreProcessMinimal, TEST_SIZE)\n",
    "val_data = read_data(val_file_X, val_file_y, SRC, SRC, PreProcessMinimal, VAL_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data.text, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_list = SRC.vocab.itos  # index2word\n",
    "src_dict = SRC.vocab.stoi # word2index\n",
    "\n",
    "PAD_IDX = SRC.vocab.stoi[SRC.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = BucketIterator(train_data, BATCH_SIZE, shuffle=True, sort_key=lambda x: len(x.text), sort_within_batch=True)\n",
    "val_iter = BucketIterator(val_data, BATCH_SIZE, sort_key=lambda x: len(x.text), sort_within_batch=True)\n",
    "test_iter = BucketIterator(test_data, BATCH_SIZE, sort_key=lambda x: len(x.text), sort_within_batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import FastText\n",
    "\n",
    "ff = FastText(\"en\")\n",
    "embeddings =  ff.get_vecs_by_tokens(SRC.vocab.itos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerSummarizer(ATTENTION_HEADS, N_LAYERS, N_LAYERS, DIM_FEEDFORWARD, SEQ_LEN, VOCAB_SIZE, PAD_IDX, embeddings=embeddings).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_batches = math.ceil(len(train_data)/BATCH_SIZE)\n",
    "val_batches = math.ceil(len(val_data)/BATCH_SIZE)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "parameters = filter(lambda p:p.requires_grad, model.parameters())\n",
    "optimizer = optim.Adam(parameters)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_iter, num_batches,optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, val_iter,val_batches, criterion, \"evaluate\")\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f}')\n",
    "    \n",
    "test_size = math.ceil(len(test_data)/BATCH_SIZE)\n",
    "test_loss = evaluate(model, test_iter, test_size, criterion, \"testing\")\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"transformer_summ.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(out_dir, \"raw.txt\"), \"w\", encoding=\"utf-8\") as text, open(os.path.join(out_dir, \"pred.txt\"), \"w\", encoding=\"utf-8\") as pred, open(os.path.join(out_dir, \"true.txt\"), \"w\", encoding=\"utf-8\") as true:\n",
    "\n",
    "    for i, batch in enumerate(test_iter):\n",
    "        if i == 1:\n",
    "            break\n",
    "        src = batch.text\n",
    "        trg = batch.summ\n",
    "        trg_inp, trg_out = trg[:-1, :], trg[1:, :]\n",
    "\n",
    "        output = model(src, trg)\n",
    "        output = F.softmax(output, dim=-1)\n",
    "        output = output_test.argmax(-1)\n",
    "\n",
    "        raw_text = \" \".join([src_list[i] for i in src.squeeze(1).transpose(0,1)[0].tolist()])\n",
    "        true_summary = \" \".join([src_list[i] for i in trg.squeeze(1).transpose(0,1)[0].tolist()])\n",
    "        prediction = \" \".join([src_list[i] for i in output.transpose(0,1)[0].tolist()])\n",
    "\n",
    "        # print(output.transpose(0,1)[0].shape)\n",
    "        # print(\"text: \", raw_text)\n",
    "        # print(\"\\n\\nsumm: \", true_summary)\n",
    "        # print(\"\\n\\npred: \", prediction)\n",
    "\n",
    "        text.write(raw_text + \"\\n\")\n",
    "        true.write(true_summary + \"\\n\")\n",
    "        pred.write(prediction + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}